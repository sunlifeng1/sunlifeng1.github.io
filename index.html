<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>孙利峰的技术博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="孙利峰的技术博客">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="孙利峰的技术博客">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="孙利峰的技术博客">
<meta name="twitter:description">
  
    <link rel="alternate" href="/atom.xml" title="孙利峰的技术博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">孙利峰的技术博客</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">天幕红尘</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-WWDC——2016" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/06/14/WWDC——2016/" class="article-date">
  <time datetime="2016-06-14T14:04:30.000Z" itemprop="datePublished">2016-06-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/06/14/WWDC——2016/">WWDC——2016</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><span></span></p>
<p>内容<br>这次的 WWDC keynote 主要分成 4 个部分，其实就是四个平台：watchOS, tvOS，macOS（原来叫 OS X）, iOS。整个分享中规中矩，分别给大家介绍一下我觉得值得关注的改进吧。<br>watchOS<br>watchOS 推出了 watchOS 3，据说启动速度更快了，但是不知道耗电是不是也是更多了。关于这个，后续有一个 session 主题分享《Architecting for Performance on watchOS 3》，感兴趣的朋友可以关注。<br>增加了 scribbles 功能，其实就是一种手写输入，手写居然还支持中文，我当时就在想：你倒是写一个复杂一点的字试试看呢？我对此功能不太看好。<br>别的改进都很小，比如增加了滑动更换主题，快速报警，现场还花大力气介绍一款叫 breathe 的应用，而这个应用就是指导你深呼吸的。不知道大家喜不喜欢，反正我是不会用的。<br>tvOS<br>这部分直接跳过吧，原因你懂的。<br>macOS<br>OS X 更名为了 macOS，可以支持通过 apple watch 解锁电脑了。哦耶，我的手表终于不再只是用来看时间了 !<br>剪切版支持从手机上复制，在电脑上粘贴，这个还是挺有用的，我平常都是用 QQ 或微信的文件传输助手做类似的事情。<br>optimize storage 功能可以把老文件自动存在云端，据说可以节省大概 100G 的空间。不过我猜想，iCloud 的免费空间应该不够用吧？是不是得花钱买些容量才行。<br>apple pay 可以在 PC 的浏览器上直接支付，点击支付时，需要在手机上确认授权。整个体验类似于拿微信或支付宝扫二维码付款。<br>siri 可以在 mac 上使用了，并且 siri 的结果可以 pin 在一个列表中，并可以拖动到别的地方。我有一阵子很喜欢用 iPhone 上的 siri，不过还是不太稳定。mac 上一般办公室是不太方便用 siri 的，家里的话要是没有培养出习惯，估计也不太会使用。所以这个功能到底实用性有多大还不确定。<br>增加了一个名为 picture in picture 的功能，其实就是看视频的时候可以固定住视频。<br>iOS<br>介绍了 10 个特性，我选一些有意思的介绍吧。<br>Siri 可以和微信整合，这个很赞，你可以直接用 siri 控制发信息给微信中的好友。<br>系统的 Map 整合了大量的应用，比如可以在上面看大众点评的评价，可以直接叫滴滴打车。如果做得好，Map 很可能成为一个重要的打车入口，想到这儿就能明白为什么苹果投了滴滴 10 亿美金了。不过就苹果系统 Map 的那个糟糕体验，我不确定我是否能够接受它。反正我已经喜欢上了高德地图里面的郭德纲和林志玲的声音，离线的地图和导航也很适合中国这种按流量计费的移动网络环境。相比而言，系统自带的 Map 有些不接地气。<br>homekit 与智能家居的整合，整体的效果非常酷，但整个还需要依赖生态产业的配合。当前来看，国内的小米在这方面做得比苹果还是要快一些。<br>电话功能终于支持恶意电话提醒了，现场提到用的是腾讯的解决方案。嗯，360 估计是没有翻身的可能了。另外，phone extension 似乎可以做更多的事情。<br>iMessage 可以发更多的内容，tap to replace to emoji 功能的交互设计还是挺有意思的，省却了选择 emoji 的时间。不过现场演示出现了 Bug，还是挺尴尬的。另外，大家都已经习惯了微信，而且 iMessage 只能和 iPhone 用户发，我觉得这些功能做得再有意思，也没有多大用。大家的直觉还是用微信来社交。<br>中国元素<br>这次 WWDC 中多次提到了中国，以及中国的互联网产品，包括微信，大众点评，滴滴。中国为苹果的赢收增长做出了巨大的贡献，我们也能看到苹果越来越重视中国区的需求了。比如恶意来电提醒，就是一个特别针对中国区用户的设计。<br>Developer<br>这次 Tim Cook 专门介绍了针对开发者做的 iPad 版的 Swift Playground，其实这哪是一个 Playground 啊，这就是一个学习编程的 3D 游戏！整个 App 设计得非常用心，效果看起来也非常棒，下面是这个应用的界面，大家感受一下。</p>
<p></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/06/14/WWDC——2016/" data-id="cipjtrim4000atjs6nncs1oiy" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-openGL学习" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/04/17/openGL学习/" class="article-date">
  <time datetime="2016-04-17T12:21:03.000Z" itemprop="datePublished">2016-04-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/04/17/openGL学习/">openGL学习</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><span></span></p>
<p>说起编程作图，大概还有很多人想起TC的#include <graphics.h>吧？</graphics.h></p>
<p>但是各位是否想过，那些画面绚丽的PC游戏是如何编写出来的？就靠TC那可怜的640*480分辨率、16色来做吗？显然是不行的。</p>
<p>本帖的目的是让大家放弃TC的老旧图形接口，让大家接触一些新事物。</p>
<p>OpenGL作为当前主流的图形API之一，它在一些场合具有比DirectX更优越的特性。</p>
<p>1、与C语言紧密结合。</p>
<p>OpenGL命令最初就是用C语言函数来进行描述的，对于学习过C语言的人来讲，OpenGL是容易理解和学习的。如果你曾经接触过TC的graphics.h，你会发现，使用OpenGL作图甚至比TC更加简单。</p>
<p>2、强大的可移植性。</p>
<p>微软的Direct3D虽然也是十分优秀的图形API，但它只用于Windows系统（现在还要加上一个XBOX游戏机）。而OpenGL不仅用于 Windows，还可以用于Unix/Linux等其它系统，它甚至在大型计算机、各种专业计算机（如：医疗用显示设备）上都有应用。并且，OpenGL 的基本命令都做到了硬件无关，甚至是平台无关。</p>
<p>3、高性能的图形渲染。</p>
<p>OpenGL是一个工业标准，它的技术紧跟时代，现今各个显卡厂家无一不对OpenGL提供强力支持，激烈的竞争中使得OpenGL性能一直领先。</p>
<p>总之，OpenGL是一个很NB的图形软件接口。至于究竟有多NB，去看看DOOM3和QUAKE4等专业游戏就知道了。</p>
<p>OpenGL官方网站（英文）</p>
<p><a href="http://www.opengl.org" target="_blank" rel="external">http://www.opengl.org</a></p>
<p>下面将对Windows下的OpenGL编程进行简单介绍。</p>
<p>学习OpenGL前的准备工作</p>
<p>第一步，选择一个编译环境</p>
<p>现在Windows系统的主流编译环境有Visual Studio，Broland C++ Builder，Dev-C++等，它们都是支持OpenGL的。但这里我们选择Visual Studio 2005作为学习OpenGL的环境。</p>
<p>第二步，安装GLUT工具包</p>
<p>GLUT不是OpenGL所必须的，但它会给我们的学习带来一定的方便，推荐安装。</p>
<p>Windows环境下的GLUT下载地址：（大小约为150k）</p>
<p><a href="http://www.opengl.org/resources/libraries/glut/glutdlls37beta.zip" target="_blank" rel="external">http://www.opengl.org/resources/libraries/glut/glutdlls37beta.zip</a></p>
<p>无法从以上地址下载的话请使用下面的连接:</p>
<p><a href="http://upload.programfan.com/upfile/200607311626279.zip" target="_blank" rel="external">http://upload.programfan.com/upfile/200607311626279.zip</a></p>
<p>Windows环境下安装GLUT的步骤：</p>
<p>1、将下载的压缩包解开，将得到5个文件</p>
<p>2、在“我的电脑”中搜索“gl.h”，并找到其所在文件夹（如果是VisualStudio2005，则应该是其安装目录下面的“VC\PlatformSDK\include\gl文件夹”）。把解压得到的glut.h放到这个文件夹。</p>
<p>3、把解压得到的glut.lib和glut32.lib放到静态函数库所在文件夹（如果是VisualStudio2005，则应该是其安装目录下面的“VC\lib”文件夹）。</p>
<p>4、把解压得到的glut.dll和glut32.dll放到操作系统目录下面的system32文件夹内。（典型的位置为：C:\Windows\System32）</p>
<p>第三步，建立一个OpenGL工程</p>
<p>这里以VisualStudio2005为例。</p>
<p>选择File-&gt;New-&gt;Project，然后选择Win32 Console Application，选择一个名字，然后按OK。</p>
<p>在谈出的对话框左边点Application Settings，找到Empty project并勾上，选择Finish。</p>
<p>然后向该工程添加一个代码文件，取名为“OpenGL.c”，注意用.c来作为文件结尾。</p>
<p>搞定了，就跟平时的工程没什么两样的。</p>
<p>第一个OpenGL程序</p>
<p>一个简单的OpenGL程序如下：（注意，如果需要编译并运行，需要正确安装GLUT，安装方法如上所述）</p>
<p>#include <gl glut.h=""></gl></p>
<p>void myDisplay(void)</p>
<p>{</p>
<p>glClear(GL_COLOR_BUFFER_BIT);</p>
<p>glRectf(-0.5f, -0.5f, 0.5f, 0.5f);</p>
<p>glFlush();</p>
<p>}</p>
<p>int main(int argc, char *argv[])</p>
<p>{</p>
<p>glutInit(&amp;argc, argv);</p>
<p>glutInitDisplayMode(GLUT_RGB | GLUT_SINGLE);</p>
<p>glutInitWindowPosition(100, 100);</p>
<p>glutInitWindowSize(400, 400);</p>
<p>glutCreateWindow(“第一个OpenGL程序”);</p>
<p>glutDisplayFunc(&amp;myDisplay);</p>
<p>glutMainLoop();</p>
<p>return 0;</p>
<p>}</p>
<p>该程序的作用是在一个黑色的窗口中央画一个白色的矩形。下面对各行语句进行说明。</p>
<p>首先，需要包含头文件#include <gl glut.h="">，这是GLUT的头文件。</gl></p>
<p>本来OpenGL程序一般还要包含<gl gl.h="">和<gl glu.h="">，但GLUT的头文件中已经自动将这两个文件包含了，不必再次包含。</gl></gl></p>
<p>然后看main函数。</p>
<p>int main(int argc, char *argv[])，这个是带命令行参数的main函数，各位应该见过吧？没见过的同志们请多翻翻书，等弄明白了再往下看。</p>
<p>注意main函数中的各语句，除了最后的return之外，其余全部以glut开头。这种以glut开头的函数都是GLUT工具包所提供的函数，下面对用到的几个函数进行介绍。</p>
<p>1、glutInit，对GLUT进行初始化，这个函数必须在其它的GLUT使用之前调用一次。其格式比较死板，一般照抄这句glutInit(&amp;argc, argv)就可以了。</p>
<p>2、 glutInitDisplayMode，设置显示方式，其中GLUT_RGB表示使用RGB颜色，与之对应的还有GLUT_INDEX（表示使用索引颜色）。GLUT_SINGLE表示使用单缓冲，与之对应的还有GLUT_DOUBLE（使用双缓冲）。更多信息，请自己Google。当然以后的教程也会有一些讲解。</p>
<p>3、glutInitWindowPosition，这个简单，设置窗口在屏幕中的位置。</p>
<p>4、glutInitWindowSize，这个也简单，设置窗口的大小。</p>
<p>5、glutCreateWindow，根据前面设置的信息创建窗口。参数将被作为窗口的标题。注意：窗口被创建后，并不立即显示到屏幕上。需要调用glutMainLoop才能看到窗口。</p>
<p>6、glutDisplayFunc，设置一个函数，当需要进行画图时，这个函数就会被调用。（这个说法不够准确，但准确的说法可能初学者不太好理解，暂时这样说吧）。</p>
<p>7、glutMainLoop，进行一个消息循环。（这个可能初学者也不太明白，现在只需要知道这个函数可以显示窗口，并且等待窗口关闭后才会返回，这就足够了。）</p>
<p>在glutDisplayFunc函数中，我们设置了“当需要画图时，请调用myDisplay函数”。于是myDisplay函数就用来画图。观察myDisplay中的三个函数调用，发现它们都以gl开头。这种以gl开头的函数都是OpenGL的标准函数，下面对用到的函数进行介绍。</p>
<p>1、glClear，清除。GL_COLOR_BUFFER_BIT表示清除颜色，glClear函数还可以清除其它的东西，但这里不作介绍。</p>
<p>2、glRectf，画一个矩形。四个参数分别表示了位于对角线上的两个点的横、纵坐标。</p>
<p>3、glFlush，保证前面的OpenGL命令立即执行（而不是让它们在缓冲区中等待）。其作用跟fflush(stdout)类似。</p>
<p></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/04/17/openGL学习/" data-id="cipjtrine000utjs6swbezxml" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-MVVM" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/04/05/MVVM/" class="article-date">
  <time datetime="2016-04-05T14:21:17.000Z" itemprop="datePublished">2016-04-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/04/05/MVVM/">MVVM</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><span></span></p>
<p>MVVM 的历史<br>MVVM 是 Model-View-ViewModel 的简写。<br>相对于 MVC 的历史来说，MVVM 是一个相当新的架构，MVVM 最早于 2005 年被微软的 WPF 和 Silverlight 的架构师 John Gossman 提出，并且应用在微软的软件开发中。当时 MVC 已经被提出了 20 多年了，可见两者出现的年代差别有多大。<br>MVVM 在使用当中，通常还会利用双向绑定技术，使得 Model 变化时，ViewModel 会自动更新，而 ViewModel 变化时，View 也会自动变化。所以，MVVM 模式有些时候又被称作：model-view-binder 模式。<br>具体在 iOS 中，可以使用 KVO 或 Notification 技术达到这种效果。<br>MVVM 的神化<br>在使用中，我发现大家对于 MVVM 以及 MVVM 衍生出来的框架（比如 ReactiveCocoa）有一种「敬畏」感。这种「敬畏」感某种程度上就像对神一样，这主要表现在我没有听到大家对于 MVVM 的任何批评。<br>我感觉原因首先是 MVVM 并没有很大程度上普及，大家对于新技术一般都不熟，进而不敢妄加评论。另外，ReactiveCocoa 本身上手的复杂性，也让很多人感觉到这种技术很高深难懂，进而加重了大家对它的「敬畏」。<br>MVVM 的作用和问题<br>MVVM 在实际使用中，确实能够使得 Model 层和 View 层解耦，但是如果你需要实现 MVVM 中的双向绑定的话，那么通常就需要引入更多复杂的框架来实现了。<br>对此，MVVM 的作者 John Gossman 的 批评 应该是最为中肯的。John Gossman 对 MVVM 的批评主要有两点：<br>第一点：数据绑定使得 Bug 很难被调试。你看到界面异常了，有可能是你 View 的代码有 Bug，也可能是 Model 的代码有问题。数据绑定使得一个位置的 Bug 被快速传递到别的位置，要定位原始出问题的地方就变得不那么容易了。<br>第二点：对于过大的项目，数据绑定需要花费更多的内存。<br>某种意义上来说，我认为就是数据绑定使得 MVVM 变得复杂和难用了。但是，这个缺点同时也被很多人认为是优点。<br>ReactiveCocoa<br>函数式编程（Functional Programming）和响应式编程（React Programming）也是当前很火的两个概念，它们的结合可以很方便地实现数据的绑定。于是，在 iOS 编程中，ReactiveCocoa 横空出世了，它的概念都非常 新，包括：<br>函数式编程（Functional Programming），函数也变成一等公民了，可以拥有和对象同样的功能，例如当成参数传递，当作返回值等。看看 Swift 语言带来的众多函数式编程的特性，就你知道这多 Cool 了。<br>响应式编程（React Programming），原来我们基于事件（Event）的处理方式都弱了，现在是基于输入（在 ReactiveCocoa 里叫 Signal）的处理方式。输入还可以通过函数式编程进行各种 Combine 或 Filter，尽显各种灵活的处理。<br>无状态（Stateless），状态是函数的魔鬼，无状态使得函数能更好地测试。<br>不可修改（Immutable），数据都是不可修改的，使得软件逻辑简单，也可以更好地测试。<br>哇，所有这些都太 Cool 了。当我看到的时候，我都鸡冻了！<br>我们应该客观评价 MVVM 和 ReactiveCocoa<br>但是但是，我突然想到，我好象只需要一个 ViewModel 而已，我完全可以简单地做一个 ViewModel 的工厂类或 Service 类就可以了，为什么要引入这么多框架？现有的 MVC 真的有那么大的问题吗？<br>直到现在，ReactiveCocoa 在国内外还都是在小众领域，没有被大量接受成为主流的编程框架。不只是在 iOS 语言，在别的语言中，例如 Java 中的 RxJava 也同样没有成为主流。<br>我在这里，不是想说 ReactiveCocoa 不好，也不是想说 MVVM 不好，而是想让大家都能够有一个客观的认识。ReactiveCocoa 和 MVVM 不应该被神化，它是一种新颖的编程框架，能够解决旧有编程框架的一些问题，但是也会带来一些新问题，仅此而已。如果不能使好的驾驭 ReactiveCocoa，同样会造成 Controller 代码过于复杂，代码逻辑不易维护的问题。</p>
<p></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/04/05/MVVM/" data-id="cipjtrilh0005tjs6xl76axh2" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-MVC" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/04/03/MVC/" class="article-date">
  <time datetime="2016-04-03T14:18:06.000Z" itemprop="datePublished">2016-04-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/04/03/MVC/">MVC</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><span></span></p>
<p>MVC 的历史<br>MVC，全称是 Model View Controller，是模型 (model)－视图 (view)－控制器 (controller) 的缩写。它表示的是一种常见的客户端软件开发框架。<br>MVC 的概念最早出现在二十世纪八十年代的 施乐帕克 实验室中（对，就是那个发明图形用户界面和鼠标的实验室），当时施乐帕克为 Smalltalk 发明了这种软件设计模式。<br>现在，MVC 已经成为主流的客户端编程框架，在 iOS 开发中，系统为我们实现好了公共的视图类：UIView，和控制器类：UIViewController。大多数时候，我们都需要继承这些类来实现我们的程序逻辑，因此，我们几乎逃避不开 MVC 这种设计模式。<br>但是，几十年过去了，我们对于 MVC 这种设计模式真的用得好吗？其实不是的，MVC 这种分层方式虽然清楚，但是如果使用不当，很可能让大量代码都集中在 Controller 之中，让 MVC 模式变成了 Massive View Controller 模式。<br>Controller 的臃肿问题何解？<br>很多人试图解决 MVC 这种架构下 Controller 比较臃肿的问题。我还记得半年前 InfoQ 搞了一次移动座谈会，当时 BeeFramework 和 Samurai-Native 的作者 老郭 问了我一句话：「什么样的内容才应该放到 Controller 中？」。但是当时因为时间不够，我没能展开我的观点，这次正好在这里好好谈谈我对于这个问题的想法。<br>我们来看看 MVC 这种架构的特点。其实设计模式很多时候是为了 Don’t repeat yourself 原则来做的，该原则要求能够复用的代码要尽量复用，来保证重用。在 MVC 这种设计模式中，我们发现 View 和 Model 都是符合这种原则的。<br>对于 View 来说，你如果抽象得好，那么一个 App 的动画效果可以很方便地移植到别的 App 上，而 Github 上也有很多 UI 控件，这些控件都是在 View 层做了很好的封装设计，使得它能够方便地开源给大家复用。<br>对于 Model 来说，它其实是用来存储业务的数据的，如果做得好，它也可以方便地复用。比如我当时在做有道云笔记 iPad 版的时候，我们就直接和 iOS 版复用了所有的 Model 层的代码。在创业做猿题库客户端时，iOS 和 iPad 版的 Model 层代码再次被复用上了。当然，因为和业务本身的数据意义相关，Model 层的复用大多数是在一个产品内部，不太可能像 View 层那样开源给社区。<br>说完 View 和 Model 了，那我们想想 Controller，Controller 有多少可以复用的？我们写完了一个 Controller 之后，可以很方便地复用它吗？结论是：非常难复用。在某些场景下，我们可能可以用 addSubViewController 之类的方式复用 Controller，但它的复用场景还是非常非常少的。<br>如果我们能够意识到 Controller 里面的代码不便于复用，我们就能知道什么代码应该写在 Controller 里面了，那就是那些不能复用的代码。在我看来，Controller 里面就只应该存放这些不能复用的代码，这些代码包括：<br>在初始化时，构造相应的 View 和 Model。<br>监听 Model 层的事件，将 Model 层的数据传递到 View 层。<br>监听 View 层的事件，并且将 View 层的事件转发到 Model 层。<br>如果 Controller 只有以上的这些代码，那么它的逻辑将非常简单，而且也会非常短。<br>但是，我们却很难做到这一点，因为还是有很多逻辑我们不知道写在哪里，于是就都写到了 Controller 中了，那我们接下来就看看其它逻辑应该写在哪里。<br>如何对 ViewController 瘦身？<br>objc.io 是一个非常有名的 iOS 开发博客，它上面的第一课 《Lighter View Controllers》 上就讲了很多这样的技巧，我们先总结一下它里面的观点：<br>将 UITableView 的 Data Source 分离到另外一个类中。<br>将数据获取和转换的逻辑分别到另外一个类中。<br>将拼装控件的逻辑，分离到另外一个类中。<br>你想明白了吗？其实 MVC 虽然只有三层，但是它并没有限制你只能有三层。所以，我们可以将 Controller 里面过于臃肿的逻辑抽取出来，形成新的可复用模块或架构层次。<br>我个人对于逻辑的抽取，有以下总结。<br>将网络请求抽象到单独的类中<br>新手写代码，直接就在 Controller 里面用 AFNetworking 发一个请求，请求的完数据直接就传递给 View。入门一些的同学，知道把这些请求代码移到另外一个静态类里面。但是我觉得还不够，所以我建议将每一个网络请求直接封装成类。<br>把每一个网络请求封装成对象其实是使用了设计模式中的 Command 模式，它有以下好处：<br>将网络请求与具体的第三方库依赖隔离，方便以后更换底层的网络库。实际上我们公司的 iOS 客户端最初是基于 ASIHttpRequest 的，我们只花了两天，就很轻松地切换到了 AFNetworking。<br>方便在基类中处理公共逻辑，例如猿题库的数据版本号信息就统一在基类中处理。<br>方便在基类中处理缓存逻辑，以及其它一些公共逻辑。<br>方便做对象的持久化。<br>大家如果感兴趣，可以看我们公司开源的 iOS 网络库：YTKNetwork。它在这种思考的指导下，不但将 Controller 中的代码瘦身，而且进一步演化和加强，现在它还支持诸如复杂网络请求管理，断点续传，插件机制，JSON 合法性检查等功能。<br>这部分代码从 Controller 中剥离出来后，不但简化了 Controller 中的逻辑，也达到了网络层的代码复用的效果。<br>将界面的拼装抽象到专门的类中<br>新手写代码，喜欢在 Controller 中把一个个 UILabel ，UIButton，UITextField 往 self.view 上用 addSubView 方法放。我建议大家可以用两种办法把这些代码从 Controller 中剥离。<br>方法一：构造专门的 UIView 的子类，来负责这些控件的拼装。这是最彻底和优雅的方式，不过稍微麻烦一些的是，你需要把这些控件的事件回调先接管，再都一一暴露回 Controller。<br>方法二：用一个静态的 Util 类，帮助你做 UIView 的拼装工作。这种方式稍微做得不太彻底，但是比较简单。<br>对于一些能复用的 UI 控件，我建议用方法一。如果项目工程比较复杂，我也建议用方法一。如果项目太紧，另外相关项目的代码量也不多，可以尝试方法二。<br>构造 ViewModel<br>谁说 MVC 就不能用 ViewModel 的？MVVM 的优点我们一样可以借鉴。具体做法就是将 ViewController 给 View 传递数据这个过程，抽象成构造 ViewModel 的过程。<br>这样抽象之后，View 只接受 ViewModel，而 Controller 只需要传递 ViewModel 这么一行代码。而另外构造 ViewModel 的过程，我们就可以移动到另外的类中了。<br>在具体实践中，我建议大家专门创建构造 ViewModel 工厂类，参见 工厂模式。另外，也可以专门将数据存取都抽将到一个 Service 层，由这层来提供 ViewModel 的获取。<br>专门构造存储类<br>刚刚说到 ViewModel 的构造可以抽奖到一个 Service 层。与此相应的，数据的存储也应该由专门的对象来做。在小猿搜题项目中，我们由一个叫 UserAgent 的类，专门来处理本地数据的存取。<br>数据存取放在专门的类中，就可以针对存取做额外的事情了。比如：<br>对一些热点数据增加缓存<br>处理数据迁移相关的逻辑<br>如果要做得更细，可以把存储引擎再抽象出一层。这样你就可以方便地切换存储的底层，例如从 sqlite 切换到 key-value 的存储引擎等。<br>小结<br>通过代码的抽取，我们可以将原本的 MVC 设计模式中的 ViewController 进一步拆分，构造出 网络请求层、ViewModel 层、Service 层、Storage 层等其它类，来配合 Controller 工作，从而使 Controller 更加简单，我们的 App 更容易维护。<br>另外，不知道大家注意到没，其实 Controller 层是非常难于测试的，如果我们能够将 Controller 瘦身，就可以更方便地写 Unit Test 来测试各种与界面的无关的逻辑。移动端自动化测试框架都不太成熟，但是将 Controller 的代码抽取出来，是有助于我们做测试工作的。</p>
<p></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/04/03/MVC/" data-id="cipjtrim20009tjs6gccc3tmn" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-UITableView的应用和其中js与oc交互" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/03/21/UITableView的应用和其中js与oc交互/" class="article-date">
  <time datetime="2016-03-21T09:11:10.000Z" itemprop="datePublished">2016-03-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/03/21/UITableView的应用和其中js与oc交互/">UITableView的应用和其中js与oc交互</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><span></span></p>
<p>现在有很多的应用已经采用了WebView和html语言结合的开发模式。html5一直很火因为一份代码可以在多个平台上运用啊，效果各不相同都很美观，也越来越有一些公司直接招后台程序员和html5程序员，做完的产品再安卓也能用iOS也能用，不用再招双份的工程师了。应用程序一进去就全是UIWebView，里面发个请求到自己用html5做的页面，这就是一个应用！当然今天的主要不是说html5，是说html语言中JS代码和OC代码之间的传值。<br>JS代码和OC代码见的传值了，一般大概思路是这边发请求，那边把请求拦下来，再扒出请求url里的字符串，再各种截取得到有用的数据</p>
<p>核心思路如下，从一半开始截的</p>
<p>NSString *onload = @”this.onclick = function() {“<br>“  window.location.href = ‘sx:src=’ +this.src;”<br>“};”;<br>[imgHtml appendFormat:@”<img onload="\"%@\"" width="\"%f\"" height="\"%f\"" src="\"%@\"">“,onload,width,height,detailImgModel.src];<br>// 结束标记<br>[imgHtml appendString:@”“];<br>// 替换标记<br>[body replaceOccurrencesOfString:detailImgModel.ref withString:imgHtml options:NSCaseInsensitiveSearch range:NSMakeRange(0, body.length)];<br>}<br>return body;</p>
<p>其中onload就是加载完毕后才用，然后onclick是JS里的点击触发，window.location.href 是跳往哪里，后面是一个url 里面的协议头是sx：src= 后面是自己图片的src。 SX是我自己的符号。</p>
<p>就相当于我自己乱写了一个协议头sx：//www.啥 ，有了协议头这就是一个跳转网页的请求 但是要注意的是，我把// 和www啥的省略了 但是前面这个sx:src=不能再省略了，如果再省略就不是协议头了，苹果就会自己给你加一个他的协议头很长很乱 后面接不住了。</p>
<p>然后写这个方法 ，webView的代理方法<br>这个方法是在即将发送请求时会被调用，返回值是一个BOOL类型，说白了就是让你控制什么请求允许发出去，什么请求拦下。<br>NSString <em>url = request.URL.absoluteString;<br>NSRange range = [url rangeOfString:@”sx:src=”];<br>if (range.location != NSNotFound) {<br>NSInteger begin = range.location + range.length;<br>NSString </em>src = [url substringFromIndex:begin];<br>[self savePictureToAlbum:src];<br>return NO;<br>}<br>return YES;</p>
<p>先取出这个请求的全url再算出sx:src=这个字符串的范围，如果是别的真正的请求，是取不到值的，就是NSNotFound，这是允许发出去的</p>
<p>如果取到range了 那就是我写的这个请求，就从这个范围往后的字符串全截取下来，这就是html代码里那个图片的地址了this.src。</p>
<p>然后有了url链接就可以保存到相册了。</p>
<p>保存到相册那个方法里，有两种选择一种是重新下载保存，一种是从缓存中取，这种比较省流量。</p>
<p>NSURLCache <em>cache =[NSURLCache sharedURLCache];<br>NSURLRequest </em>request = [NSURLRequest requestWithURL:[NSURL URLWithString:src]];<br>NSData <em>imgData = [cache cachedResponseForRequest:request].data;<br>UIImage </em>image = [UIImage imageWithData:imgData];<br>UIImageWriteToSavedPhotosAlbum(image, nil, nil, nil);</p>
<p>应该都能看懂的吧，大意就是通过这个url找到我上次请求中用这个url弄到的响应数据再转化成图片保存入相册。</p>
<p>到此就完成了JS与OC间的传值,当然如果只有这样那还远远不够,这里是只有一个保存图片的方法，如果以后又有很多需要拦截的方法，打电话或者发短信等各种各样的方法，都要一一判断就太麻烦了。</p>
<p>聪明的做法是把 需要调用的方法名 和 要传的参数 都通通写在url里让我拦</p>
<p>比如sx:call:&amp;10086 </p>
<p>然后我通过字符串的切割就可以 得到一个方法名 call：和一个参数10086</p>
<p>就可以调用- (void)call:(NSString *)phoneNumber  这个方法了。</p>
<p>NSRange range = [url rangeOfString:@”sx:”];<br>if (range.location != NSNotFound) {<br>NSUInteger loc = range.location + range.length;<br>NSString <em>path = [url substringFromIndex:loc];<br>// 获得方法和参数<br>NSArray </em>methodNameAndParam = [path componentsSeparatedByString:@”&amp;”];<br>// 方法名<br>NSString *methodName = [methodNameAndParam firstObject];<br>// 调用方法<br>SEL selector = NSSelectorFromString(methodName);<br>if ([self respondsToSelector:selector]) { // 判断方法的目的： 防止因为方法不存在而报错</p>
<p>这一串代码写的很清楚了，获得方法名和参数，把字符串转化成SEL 然后下面就可以这么写了</p>
<p>调用打电话方法 call 传的值是 10086</p>
<p>如果遇到了有的方法需要传多个值。那就该这么写</p>
<p>sx:sendMsg:body:&amp;18686652446&amp;loveyou</p>
<p>通过切割可以得到方法名是 sendMsg：body：  要传的参数用&amp;切开 就是这个（componentsSeparatedByString:@”&amp;”）得到两个参数电话号码和信息内容</p>
<p>然后就可以调用这个方法了</p>
<p>这里我要说一下<br>如果要传入3个4个好多个参数，要用到一个第三方框架</p>
<p>//  Copyright (c) 2015年 shangxianDante. All rights reserved.<br>//</p>
<p>#import <foundation foundation.h=""></foundation></p>
<p>@interface NSObject (Extension)</p>
<ul>
<li>(id)performSelector:(SEL)selector withObjects:(NSArray *)objects;</li>
</ul>
<p>@end<br>//  Copyright (c) 2015年 shangxianDante. All rights reserved.<br>//</p>
<p>#import “NSObject+Extension.h”</p>
<p>@implementation NSObject (Extension)</p>
<ul>
<li>(id)performSelector:(SEL)selector withObjects:(NSArray <em>)objects {<br>NSMethodSignature </em>signature = [self methodSignatureForSelector:selector];<br>if (signature) {<br>NSInvocation* invocation = [NSInvocation invocationWithMethodSignature:signature];<br>[invocation setTarget:self];<br>[invocation setSelector:selector];<br>for(int i = 0; i &lt; [objects count]; i++){<br>id object = [objects objectAtIndex:i];<br>[invocation setArgument:&amp;object atIndex: (i + 2)];<br>}<br>[invocation invoke];<br>if (signature.methodReturnLength) {<br>id anObject;<br>[invocation getReturnValue:&amp;anObject];<br>return anObject;<br>} else {<br>return nil;<br>}<br>} else {<br>return nil;<br>}<br>}<br>@end</li>
</ul>
<p>然后就可以用这个方法啦</p>
<p></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/03/21/UITableView的应用和其中js与oc交互/" data-id="cipjtrilu0008tjs67mylh5f8" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-搞定iOS与js交互" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/02/15/搞定iOS与js交互/" class="article-date">
  <time datetime="2016-02-15T14:38:11.000Z" itemprop="datePublished">2016-02-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/02/15/搞定iOS与js交互/">搞定iOS与js交互</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><span></span></p>
<p>先看这篇文章： <a href="http://www.open-open.com/lib/view/open1419305655562.html" target="_blank" rel="external">http://www.open-open.com/lib/view/open1419305655562.html</a><br><a href="http://blog.csdn.net/lizhongfu2013/article/details/9232129" target="_blank" rel="external">http://blog.csdn.net/lizhongfu2013/article/details/9232129</a><br><a href="http://blog.csdn.net/lizhongfu2013/article/details/9236357" target="_blank" rel="external">http://blog.csdn.net/lizhongfu2013/article/details/9236357</a><br>干货好文章 <a href="http://www.jianshu.com/p/a329cd4a67ee" target="_blank" rel="external">http://www.jianshu.com/p/a329cd4a67ee</a><br>干货好文章 <a href="http://ios.jobbole.com/84491/" target="_blank" rel="external">http://ios.jobbole.com/84491/</a><br>交互范例 ：<a href="http://www.open-open.com/lib/view/open1463553967635.html" target="_blank" rel="external">http://www.open-open.com/lib/view/open1463553967635.html</a><br><a href="http://www.open-open.com/lib/view/open1456311816823.html" target="_blank" rel="external">http://www.open-open.com/lib/view/open1456311816823.html</a></p>
<p></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/02/15/搞定iOS与js交互/" data-id="cipjtrinr0011tjs63ka7tskq" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-移动直播开发技术介绍" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/01/28/移动直播开发技术介绍/" class="article-date">
  <time datetime="2016-01-28T14:29:41.000Z" itemprop="datePublished">2016-01-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/01/28/移动直播开发技术介绍/">移动直播开发技术介绍</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><span></span></p>
<p>现今移动直播技术上的挑战要远远难于传统设备或电脑直播，其完整的处理环节包括但不限于：音视频采集、美颜/滤镜/特效处理、编码、封包、推流、转码、分发、解码/渲染/播放等。</p>
<p>直播常见的问题包括</p>
<p>主播在不稳定的网络环境下如何稳定推流？<br>偏远地区的观众如何高清流畅观看直播？<br>直播卡顿时如何智能切换线路？<br>如何精确度量直播质量指标并实时调整？<br>移动设备上不同的芯片平台如何高性能编码和渲染视频？<br>美颜等滤镜特效处理怎么做？<br>如何实现播放秒开？<br>如何保障直播持续播放流畅不卡顿？</p>
<p>本次分享将为大家揭开移动直播核心技术的神秘面纱。</p>
<p>视频、直播等基础知识</p>
<p>什么是视频？</p>
<p>首先我们需要理解一个最基本的概念：视频。从感性的角度来看，视频就是一部充满趣味的影片，可以是电影，可以是短片，是一连贯的视觉冲击力表现丰富的画面和音频。但从理性的角度来看，视频是一种有结构的数据，用工程的语言解释，我们可以把视频剖析成如下结构：</p>
<p>内容元素 ( Content )<br>图像 ( Image )<br>音频 ( Audio )<br>元信息 ( Metadata ) </p>
<p>编码格式 ( Codec )<br>Video : H.264，H.265, …<br>Audio : AAC， HE-AAC, …</p>
<p>容器封装 (Container)<br>MP4，MOV，FLV，RM，RMVB，AVI，…</p>
<p>任何一个视频 Video 文件，从结构上讲，都是这样一种组成方式：</p>
<p>由图像和音频构成最基本的内容元素；<br>图像经过视频编码压缩格式处理（通常是 H.264）；<br>音频经过音频编码压缩格式处理（例如 AAC）；<br>注明相应的元信息（Metadata）；</p>
<p>最后经过一遍容器（Container）封装打包（例如 MP4），构成一个完整的视频文件。</p>
<p>如果觉得难以理解，可以想象成一瓶番茄酱。最外层的瓶子好比这个容器封装（Container），瓶子上注明的原材料和加工厂地等信息好比元信息（Metadata），瓶盖打开（解封装）后，番茄酱本身好比经过压缩处理过后的编码内容，番茄和调料加工成番茄酱的过程就好比编码（Codec），而原材料番茄和调料则好比最原本的内容元素（Content）。</p>
<p>视频的实时传输</p>
<p>简而言之，理性的认知视频的结构后，有助于我们理解视频直播。如果视频是一种“有结构的数据”，那么视频直播无疑是实时传输这种“有结构的数据”（视频）的方式。</p>
<p>那么一个显而易见的问题是：如何实时（Real-Time）传输这种“有结构的数据”（视频）呢？</p>
<p>这里边一个悖论是：一个经过容器（Container）封装后的视频，一定是不可变的 ( Immutable ) 视频文件，不可变的 ( Immutable ) 的视频文件已经是一个生产结果，根据“相对论”，而这个生产结果显然不可能精确到实时的程度，它已经是一段时空的记忆。</p>
<p>因此视频直播，一定是一个 “边生产，边传输，边消费”的过程。这意味着，我们需要更近一步了解视频从原始的内容元素 ( 图像和音频 ) 到成品 ( 视频文件 ) 之前的中间过程 ( 编码 )。</p>
<p>视频编码压缩</p>
<p>不妨让我们来深入浅出理解视频编码压缩技术。</p>
<p>为了便于视频内容的存储和传输，通常需要减少视频内容的体积，也就是需要将原始的内容元素(图像和音频)经过压缩，压缩算法也简称编码格式。例如视频里边的原始图像数据会采用 H.264 编码格式进行压缩，音频采样数据会采用 AAC 编码格式进行压缩。</p>
<p>视频内容经过编码压缩后，确实有利于存储和传输; 不过当要观看播放时，相应地也需要解码过程。因此编码和解码之间，显然需要约定一种编码器和解码器都可以理解的约定。就视频图像编码和解码而言，这种约定很简单：</p>
<p>编码器将多张图像进行编码后生产成一段一段的 GOP ( Group of Pictures ) ， 解码器在播放时则是读取一段一段的 GOP 进行解码后读取画面再渲染显示。</p>
<p>GOP ( Group of Pictures ) 是一组连续的画面，由一张 I 帧和数张 B / P 帧组成，是视频图像编码器和解码器存取的基本单位，它的排列顺序将会一直重复到影像结束。</p>
<p>I 帧是内部编码帧（也称为关键帧），P 帧是前向预测帧（前向参考帧），B 帧是双向内插帧（双向参考帧）。简单地讲，I 帧是一个完整的画面，而 P 帧和 B 帧记录的是相对于 I 帧的变化。</p>
<p>如果没有 I 帧，P 帧和 B 帧就无法解码。</p>
<p>小结一下，一个视频 ( Video ) ，其图像部分的数据是一组 GOP 的集合, 而单个 GOP 则是一组 I / P / B 帧图像的集合。</p>
<p>在这样的一种几何关系中，Video 好比一个 “物体”，GOP 好比 “分子”，I / P / B 帧的图像则好比 “原子”。</p>
<p>想象一下，如果我们把传输一个 “物体”，改成传输一个一个的 “原子”，将最小颗粒以光速传送，那么以人的生物肉眼来感知，将是一种怎样的体验？</p>
<p>什么是视频直播？</p>
<p>不难脑洞大开一下，直播就是这样的一种体验。视频直播技术，就是将视频内容的最小颗粒 ( I / P / B 帧，…)，基于时间序列，以光速进行传送的一种技术。</p>
<p>简而言之，直播就是将每一帧数据 ( Video / Audio / Data Frame )，打上时序标签 ( Timestamp ) 后进行流式传输的过程。发送端源源不断的采集音视频数据，经过编码、封包、推流，再经过中继分发网络进行扩散传播，播放端再源源不断地下载数据并按时序进行解码播放。如此就实现了 “边生产、边传输、边消费” 的直播过程。</p>
<p>理解以上两个关于 视频 和 直播 两个基础概念后，接下来我们就可以一窥直播的业务逻辑了。</p>
<p>直播的业务逻辑</p>
<p>如下是一个最精简的一对多直播业务模型，以及各个层级之间的协议。</p>
<p>各协议差异对比如下</p>
<p>以上就是关于直播技术的一些基础概念。下面我们进一步了解下影响人们视觉体验的直播性能指标。</p>
<p>影响视觉体验的直播性能指标</p>
<p>直播第一个性能指标是延迟，延迟是数据从信息源发送到目的地所需的时间。</p>
<p>根据爱因斯坦的狭义相对论，光速是所有能量、物质和信息运动所能达到的最高速度，这个结论给传播速度设定了上限。因此，即便我们肉眼感觉到的实时，实际上也是有一定的延迟。</p>
<p>由于 RTMP/HLS 是基于 TCP 之上的应用层协议，TCP 三次握手，四次挥手，慢启动过程中的每一次往返来回，都会加上一次往返耗时 ( RTT )，这些交互过程都会增加延迟。</p>
<p>其次根据 TCP 丢包重传特性，网络抖动可能导致丢包重传，也会间接导致延迟加大。</p>
<p>一个完整的直播过程，包括但不限于以下环节：采集、处理、编码、封包、推流、传输、转码、分发、拉流、解码、播放。从推流到播放，再经过中间转发环节，延迟越低，则用户体验越好。</p>
<p>第二个直播性能指标卡顿，是指视频播放过程中出现画面滞帧，让人们明显感觉到“卡”。单位时间内的播放卡顿次数统计称之为卡顿率。</p>
<p>造成卡顿的因素有可能是推流端发送数据中断，也有可能是公网传输拥塞或网络抖动异常，也有可能是终端设备的解码性能太差。卡顿频次越少或没有，则说明用户体验越好。</p>
<p>第三个直播性能指标首屏耗时，指第一次点击播放后，肉眼看到画面所等待的时间。技术上指播放器解码第一帧渲染显示画面所花的耗时。通常说的 “秒开”，指点击播放后，一秒内即可看到播放画面。首屏打开越快，说明用户体验越好。</p>
<p>如上三个直播性能指标，分别对应一个低延迟、高清流畅、极速秒开 的用户体验诉求。了解这三个性能指标，对优化移动直播 APP 的用户体验至关重要。</p>
<p>那么移动直播场景下具体而言有哪些常见的坑呢？</p>
<p>根据实践总结下来的经验，移动平台上视频直播的坑主要可以总结为两方面：设备差异，以及网络环境这些场景下带来的技术考验。</p>
<p>移动直播场景的坑与规避措施</p>
<p>不同芯片平台上的编码差异</p>
<p>iOS 平台上无论硬编还是软编，由于是 Apple 一家公司出厂，几乎不存在因为芯片平台不同而导致的编码差异。</p>
<p>然而，在 Android 平台上，Android Framework SDK 提供的 MediaCodec 编码器，在不同的芯片平台上，差异表现很大， 不同的厂家使用不同的芯片，而不同的芯片平台上 Android MediaCodec 表现略有差异，通常实现全平台兼容的成本不低。</p>
<p>另外就是 Android MediaCodec 硬编层面的 H.264 编码画质参数是固定的 baseline，所以画质通常也一般。因此，在 Android 平台下，推荐是用软编，好处是画质可调控，兼容性也更好。</p>
<p>低端设备如何上高性能地采集和编码？</p>
<p>例如 Camera 采集输出的可能是图片，一张图的体积并不会小，如果采集的频次很高，编码的帧率很高，每张图都经过编码器，那么编码器又可能会出现过载。</p>
<p>这个时候，可以考虑在编码前，不影响画质的前提下（前面我们讲过帧率的微观意义），进行选择性丢帧，以此降低编码环节的功耗开销。</p>
<p>弱网下如何保障高清流畅推流</p>
<p>移动网络下，通常容易遇到网络不稳定，连接被重置，断线重连，一方面频繁重连，建立连接需要开销。另一方面尤其是发生 GPRS / 2G / 3G / 4G 切换时，带宽可能出现瓶颈。当带宽不够，帧率较高/码率较高的内容较难发送出去，这个时候就需要可变码率支持。</p>
<p>即在推流端，可检测网络状态和简单测速，动态来切换码率，以保障网络切换时的推流流畅。</p>
<p>其次编码、封包、推流 这一部分的逻辑也可以做微调，可以尝试选择性丢帧，比如优先丢视频参考帧（不丢 I 帧和音频帧 )，这样也可以减少要传输的数据内容，但同时又达到了不影响画质和版视听流畅的目的。</p>
<p>需要区分直播流的状态和业务状态</p>
<p>直播是媒体流、APP 的交互是 API 信令流，两者的状态不能混为一谈。尤其是不能基于 APP 的交互的 API 状态来判断直播流的状态。</p>
<p>以上是移动直播场景下常见的几个坑和规避措施。</p>
<p>移动直播场景其他优化措施</p>
<p>一、怎么优化打开速度，达到传说中的 “秒开”？</p>
<p>大家可能会看到，市面上某些手机直播 APP 的打开速度非常快，一点就开。而某些手机直播 APP，点击播放后要等好几秒以后才能播放。是什么原因导致如此的天壤之别呢？</p>
<p>大部分播放器都是拿到一个完成的 GOP 后才能解码播放，基于 FFmpeg 移植的播放器甚至需要等待音画时间戳同步后才能播放（如果一个直播里边没有音频只有视频相当于要等待音频超时后才能播放画面）。</p>
<p>“秒开”可以从以下几个方面考虑：</p>
<ol>
<li>改写播放器逻辑让播放器拿到第一个关键帧后就给予显示。</li>
</ol>
<p>GOP 的第一帧通常都是关键帧，由于加载的数据较少，可以达到 “首帧秒开”。</p>
<p>如果直播服务器支持 GOP 缓存，意味着播放器在和服务器建立连接后可立即拿到数据，从而省却跨地域和跨运营商的回源传输时间。</p>
<p>GOP 体现了关键帧的周期，也就是两个关键帧之间的距离，即一个帧组的最大帧数。假设一个视频的恒定帧率是 24fps（即1秒24帧图像），关键帧周期为 2s，那么一个 GOP 就是 48 张图像。一般而言，每一秒视频至少需要使用一个关键帧。</p>
<p>增加关键帧个数可改善画质（GOP 通常为 FPS 的倍数），但是同时增加了带宽和网络负载。这意味着，客户端播放器下载一个 GOP，毕竟该 GOP 存在一定的数据体积，如果播放端网络不佳，有可能不是能够快速在秒级以内下载完该 GOP，进而影响观感体验。</p>
<p>如果不能更改播放器行为逻辑为首帧秒开，直播服务器也可以做一些取巧处理，比如从缓存 GOP 改成缓存双关键帧（减少图像数量），这样可以极大程度地减少播放器加载 GOP 要传输的内容体积。</p>
<ol>
<li>在 APP 业务逻辑层面方面优化。</li>
</ol>
<p>比如提前做好 DNS 解析（省却几十毫秒），和提前做好测速选线（择取最优线路）。经过这样的预处理后，在点击播放按钮时，将极大提高下载性能。</p>
<p>一方面，可以围绕传输层面做性能优化；另一方面，可以围绕客户播放行为做业务逻辑优化。两者可以有效的互为补充，作为秒开的优化空间。</p>
<p>二、美颜等滤镜如何处理？</p>
<p>在手机直播场景下，这就是一个刚需。没有美颜功能的手机直播 APP，主播基本不爱用。可以在采集画面后，将数据送给编码器之前，将数据源回调给滤镜处理程序，原始数据经过滤镜处理完后，再送回给编码器进行编码即可。</p>
<p>除了移动端可以做体验优化之外，直播流媒体服务端架构也可以降低延迟。例如收流服务器主动推送 GOP 至边缘节点，边缘节点缓存 GOP，播放端则可以快速加载，减少回源延迟。</p>
<p>其次，可以贴近终端就近处理和分发</p>
<p>三、如何保障直播持续播放流畅不卡顿？</p>
<p>“秒开”解决的是直播首次加载的播放体验，如何保障直播持续播放过程中的画面和声音视听流畅呢？因为，一个直播毕竟不是一个 HTTP 一样的一次性请求，而是一个 Socket 层面的长连接维持，直到直到主播主动终止推流。</p>
<p>上述我们讲过卡顿的定义：即播放时画面滞帧，触发了人们的视觉感受。在不考虑终端设备性能差异的情况下，针对网络传输层面的原因，我们看看如何保障一个持续的直播不卡顿。</p>
<p>这其实是一个直播过程中传输网络不可靠时的容错问题。例如，播放端临时断网了，但又快速恢复了，针对这种场景，播放端如果不做容错处理，很难不出现黑屏或是重新加载播放的现象。</p>
<p>为了容忍这种网络错误，并达到让终端用户无感知，客户端播放器可以考虑构建一个FIFO（先进先出）的缓冲队列，解码器从播放缓存队列读取数据，缓存队列从直播服务器源源不断的下载数据。通常，缓存队列的容量是以时间为单位（比如3s），在播放端网络不可靠时，客户端缓存区可以起到“断网无感”的过渡作用。</p>
<p>显然，这只是一个“缓兵之计”，如果直播服务器边缘节点出现故障，而此时客户端播放器又是长连接，在无法收到对端的连接断开信号，客户端的缓冲区容量再大也不管用了，这个时候就需要结合客户端业务逻辑来做调度。</p>
<p>重要的是客户端结合服务端，可以做精准调度。在初始化直播推流之前，例如基于 IP 地理位置和运营商的精确调度，分配线路质量最优的边缘接入节点。在直播推流的过程中，可以实时监测帧率反馈等质量数据，基于直播流的质量动态调整线路。</p>
<p>Q &amp; A</p>
<ol>
<li><p>关键帧设置频率一般是多少？有没有根据接入动态设置？过长首屏秒会很难做到。<br>徐立：关键帧间隔越长，也就是 GOP 越长，理论上画面越高清。但是生成 HLS 直播时，最小切割粒度也是一个 GOP，所以针对交互直播，通常不建议 GOP 设置太长。直播一般 2 个关键帧间隔即可。比如帧率是 24fps， 那么 2 个关键帧的间隔就是 48fps ，这个 GOP 就是2s。</p>
</li>
<li><p>七牛这个直播是用的网宿加速？有遇到什么坑没？<br>徐立：七牛在直播方面主要是自建节点，也支持融合众多第三方 CDN 服务商，多样化的线路组合为客户提供更优质的服务。在和第三方 CDN 合作的过程中遇到的问题等有机会再做更细粒度的交流和分享。</p>
</li>
<li><p>RTMP 直播流除了优化线路外，还有什么加速手段吗？<br>徐立：物理上优化线路，逻辑上优化策略，比如选择性丢帧，不影响编码画质的前提下减轻传输体积。</p>
</li>
<li><p>OBS 推流，播放端 HLS 出现视/音频不同步是哪个环节的问题？怎么优化？<br>徐立：有可能是采集端的问题，如果是采集端编码环节就出现音画不同步，可以在收流服务器上做音画时间戳同步，这样是全局的校对。如果是播放端解码性能问题，那么需要调节播放逻辑，比如保证音画时间戳强一致性的前提下，选择性丢一部帧。</p>
</li>
<li><p>PPT 前几页中一个概念好像错了，I 帧不是关键帧，IDR 帧才是。IDR 帧是 I 帧，但是 I 帧不一定是 IDR 帧。只有 IDR 帧才是可重入的。<br>徐立：中文都把 I 帧翻译成关键帧了，不过既然提到了 IDR 帧，可以展开说明一下。所有的 IDR 帧都是 I 帧，但是并不是所有 I 帧都是 IDR 帧，IDR 帧是 I 帧的子集。I 帧严格定义是帧内编码帧，由于是一个全帧压缩编码帧，通常用 I 帧表示 “关键帧”。IDR 是基于 I 帧的一个 “扩展”，带了控制逻辑，IDR 图像都是 I 帧图像，当解码器解码到 IDR 图像时，会立即将参考帧队列清空，将已解码的数据全部输出或抛弃。重新查找参数集，开始一个新的序列。这样如果前一个序列出现重大错误，在这里可以获得重新同步的机会。IDR 图像之后的图像永远不会使用 IDR 之前的图像的数据来解码。</p>
</li>
<li><p>有没有调研过 nginx rtmp module，为什么没有用，对它有什么评价？<br>徐立：有调研过，nginx_rtmp_module 是单进程多线程，非 go 这种轻量级线程/协程用并发自然语义的方式编写流业务。nginx 原本的代码量较大（约 16 万行，但和直播业务相关的功能并不是很多）。且主要靠写 nginx.conf 做配置租户，通常单租户可以，但业务可扩展性方面不是很灵活，可满足基本需求，不满足高级功能。</p>
</li>
<li><p>用到了那些开源软件？编码用的是 x264 吗？直播服务器你们自己开发还是开源的？<br>徐立：直播服务器用 go 开发的，移动端编码优先硬编，软编用 x264</p>
</li>
<li><p>请教一下用 OBS 推流到 nginx_rtmp_module 的时候是已经做了视频压缩了还是需要基于 OBS 再开发？<br>徐立：OBS 把编码压缩都做了，不需要再开发。</p>
</li>
<li><p>视频直播想在 HLS 流中无缝插入一段广告的 ts 文件，有问题想请教一下：1、这段 ts 的分辨率是否一定要和之前的视频流一致？2、pts 时间戳是否要和上一个 ts 递增？<br>徐立：1、可以不一致。这种情况两段视频完全是独立状态，可以没有任何关系，只需要插入 discontinue 标记，播放器在识别到这个标记之后重置解码器参数就可以无缝播放，画面会很平滑的切换。2、不需要递增。举个例子，视频 A 正在直播，播放到 pts 在 5s 的时候，插入一个视频 B，需要先插入一个 discontinue，再插入 B，等 B 播放完之后，再插入一个 discontinue，再插入 A，这个时候 A 的 pts 可以和之前递增，也可以按照中间插入的 B 的时长做偏移，一般做点播和时移的时候 pts 会连续递增，直播的话会算上 B 的时长。</p>
</li>
</ol>
<p>PPT 下载地址</p>
<p><a href="http://77fycs.com2.z0.glb.qiniucdn.com/pili_technology_sharing.pdf" target="_blank" rel="external">http://77fycs.com2.z0.glb.qiniucdn.com/pili_technology_sharing.pdf</a></p>
<p>由于移动直播在实践上还有非常多细节，本文未能全部覆盖，感兴趣的朋友欢迎在文章最后留言讨论。</p>
<p></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/01/28/移动直播开发技术介绍/" data-id="cipjtrio80014tjs6upnrtrl1" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Bilibili开源的直播框架" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/01/22/Bilibili开源的直播框架/" class="article-date">
  <time datetime="2016-01-22T14:20:09.000Z" itemprop="datePublished">2016-01-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/01/22/Bilibili开源的直播框架/">Bilibili开源的直播框架</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><span></span></p>
<p>文档地址：<a href="http://www.jianshu.com/p/1f06b27b3ac0" target="_blank" rel="external">http://www.jianshu.com/p/1f06b27b3ac0</a></p>
<p>ijkplayer 是一款做视频直播的框架, 基于ffmpeg, 支持 Android 和 iOS, 网上也有很多集成说明, 但是个人觉得还是不够详细, 在这里详细的讲一下在 iOS 中如何集成ijkplayer, 即便以前从没有接触过, 按着下面做也可以集成成功!</p>
<p>一. 下载ijkplayer<br>ijkplayer下载地址:<a href="https://github.com/Bilibili/ijkplayer" target="_blank" rel="external">https://github.com/Bilibili/ijkplayer</a><br>下载完成后解压</p>
<p>二. 编译 ijkplayer<br>说是编译 ijkplayer, 其实是编译 ffmpeg, 在这里我们已经下载好了ijkplayer, 所以 github 上README.md中的Build iOS那一步中有一些步骤是不需要的.<br>下面开始一步一步编译:<br>1.打开终端, cd 到jkplayer-master文件夹中, 也就是下载完解压后的文件夹</p>
<p>2.执行命令行./init-ios.sh, 这一步是去下载 ffmpeg 的, 时间会久一点, 耐心等一下</p>
<p>3.在第2步中下载完成后, 执行cd ios, 也就是进入到 ios目录中</p>
<p>4.进入 ios 文件夹后, 在终端依次执行./compile-ffmpeg.sh clean和./compile-ffmpeg.sh all命令, 编译 ffmpeg, 也就是README.md中这两步</p>
<p>编译 ffmpeg<br>编译时间较久, 耐心等待一下.</p>
<p>三. 打包IJKMediaFramework.framework框架<br>集成 ijkplayer 有两种方法:<br>一种方法是按照IJKMediaDemo工程中那样, 直接导入工程IJKMediaPlayer.xcodeproj, 在这里不做介绍</p>
<p>导入IJKMediaPlayer.xcodeproj<br>第二种集成方法是把 ijkplayer 打包成framework导入工程中使用. 下面开始介绍如何打包IJKMediaFramework.framework, 按下面步骤开始一步一步做:</p>
<p>首先打开工程IJKMediaPlayer.xcodeproj<br>要打包的 framework 工程.png<br>2.工程打开后设置工程的 scheme</p>
<p>3.设置好 scheme 后, 分别选择真机和模拟器进行编译, 编译完成后, 进入 Finder</p>
<p>进入 Finder 后, 可以看到有真机和模拟器两个版本的编译结果</p>
<p>运行后生成的文件.png<br>下面开始合并真机和模拟器版本的 framework, 注意不要合并错了, 合并的是这个文件</p>
<p>合并真机和模拟器文件中的这个文件.png<br>打开终端, 进行合并, 命令行具体格式为:</p>
<p>lipo -create “真机版本路径” “模拟器版本路径” -output “合并后的文件路径”<br>合并</p>
<p>合并生成后的文件.png<br>下面很重要, 需要用合并后的IJKMediaFramework把原来的IJKMediaFramework替换掉, 如下图, 希望你能看懂:</p>
<p>用合并生成的文件替换原来的文件.png<br>上图中的1、2两步完成后, 绿色框住的那个IJKMediaFramework.framework文件就是我们需要的框架了, 可以复制出来, 稍后我们需要导入工程使用.</p>
<p>四. iOS工程中集成ijkplayer<br>新建工程, 导入合并后的IJKMediaFramework.framework以及相关依赖框架以及相关依赖框架</p>
<p>导入 framework及依赖框架.png<br>导入框架后, 在ViewController.m进行测试, 首先导入IJKMediaFramework.h头文件, 编译看有没有错, 如果没有错说明集成成功.<br>接着开始在ViewController.m文件中使用IJKMediaFramework框架进行测试使用, 写一个简单的直播视频进行测试, 在这里看一下运行后的结果, 后面会放上 Demo 供下载.</p>
<p>文／jianshu_wl（简书作者）<br>原文链接：<a href="http://www.jianshu.com/p/1f06b27b3ac0" target="_blank" rel="external">http://www.jianshu.com/p/1f06b27b3ac0</a><br>著作权归作者所有，转载请联系作者获得授权，并标注“简书作者”。</p>
<p></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/01/22/Bilibili开源的直播框架/" data-id="cipjtril30001tjs67dnbruz6" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-iOS三种视屏录制方式" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/01/14/iOS三种视屏录制方式/" class="article-date">
  <time datetime="2016-01-14T14:14:49.000Z" itemprop="datePublished">2016-01-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/01/14/iOS三种视屏录制方式/">iOS三种视屏录制方式</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><span></span></p>
<p>随着每一代 iPhone 处理能力和相机硬件配置的提高，使用它来捕获视频也变得更加有意思。它们小巧，轻便，低调，而且与专业摄像机之间的差距已经变得非常小，小到在某些情况下，iPhone 可以真正替代它们。</p>
<p>这篇文章讨论了关于如何配置视频捕获管线 (pipeline) 和最大限度地利用硬件性能的一些不同选择。 这里有个使用了不同管线的样例 app，可以在 GitHub 查看。</p>
<p>UIImagePickerController</p>
<p>目前，将视频捕获集成到你的应用中的最简单的方法是使用 UIImagePickerController。这是一个封装了完整视频捕获管线和相机 UI 的 view controller。</p>
<p>在实例化相机之前，首先要检查设备是否支持相机录制：</p>
<p>Objective-C</p>
<p>if ([UIImagePickerController<br>isSourceTypeAvailable:UIImagePickerControllerSourceTypeCamera]) {<br>NSArray <em>availableMediaTypes = [UIImagePickerController<br>availableMediaTypesForSourceType:UIImagePickerControllerSourceTypeCamera];<br>if ([availableMediaTypes containsObject:(NSString </em>)kUTTypeMovie]) {<br>// 支持视频录制<br>}<br>}<br>然后创建一个 UIImagePickerController 对象，设置好代理便于进一步处理录制好的视频 (比如存到相册) 以及对于用户关闭相机作出响应：</p>
<p>Objective-C</p>
<p>UIImagePickerController <em>camera = [UIImagePickerController new];<br>camera.sourceType = UIImagePickerControllerSourceTypeCamera;<br>camera.mediaTypes = @[(NSString </em>)kUTTypeMovie];<br>camera.delegate = self;<br>这是你实现一个功能完善的摄像机所需要写的所有代码。</p>
<p>相机配置</p>
<p>UIImagePickerController 提供了额外的配置选项。</p>
<p>通过设置 cameraDevice 属性可以选择一个特定的相机。这是一个 UIImagePickerControllerCameraDevice 枚举，默认情况下是 UIImagePickerControllerCameraDeviceRear，你也可以把它设置为 UIImagePickerControllerCameraDeviceFront。每次都应事先确认你想要设置的相机是可用的：</p>
<p>Objective-C</p>
<p>UIImagePickerController *camera = …<br>if ([UIImagePickerController isCameraDeviceAvailable:UIImagePickerControllerCameraDeviceFront]) {<br>[camera setCameraDevice:UIImagePickerControllerCameraDeviceFront];<br>}<br>videoQuality 属性用于控制录制视频的质量。它允许你设置一个特定的编码预设，从而改变视频的比特率和分辨率。以下是六种预设：</p>
<p>Objective-C</p>
<p>enum {<br>UIImagePickerControllerQualityTypeHigh             = 0,<br>UIImagePickerControllerQualityTypeMedium           = 1,  // default  value<br>UIImagePickerControllerQualityTypeLow              = 2,<br>UIImagePickerControllerQualityType640x480          = 3,<br>UIImagePickerControllerQualityTypeIFrame1280x720   = 4,<br>UIImagePickerControllerQualityTypeIFrame960x540    = 5<br>};<br>typedef NSUInteger  UIImagePickerControllerQualityType;<br>前三种为相对预设 (low, medium, high)。这些预设的编码配置会因设备不同而不同。如果选择 high，那么你选定的相机会提供给你该设备所能支持的最高画质。后面三种是特定分辨率的预设 (640×480 VGA, 960×540 iFrame, 和 1280×720 iFrame)。</p>
<p>自定义 UI</p>
<p>就像上面提到的，UIImagePickerController 自带一套相机 UI，可以直接使用。然而，你也可以自定义相机的控件，通过隐藏默认控件，然后创建带有控件的自定义视图，并覆盖在相机预览图层上面：</p>
<p>Objective-C</p>
<p>UIView *cameraOverlay = …<br>picker.showsCameraControls = NO;<br>picker.cameraOverlayView = cameraOverlay;<br>然后你需要将你覆盖层上的控件关联上 UIImagePickerController 的控制方法 (比如，startVideoCapture 和 stopVideoCapture)。</p>
<p>AVFoundation</p>
<p>如果你想要更多关于处理捕获视频的方法，而这些方法是 UIImagePickerController 所不能提供的，那么你需要使用 AVFoundation。</p>
<p>AVFoundation 中关于视频捕获的主要的类是 AVCaptureSession。它负责调配影音输入与输出之间的数据流：</p>
<p>AVCaptureSession setup<br>使用一个 capture session，你需要先实例化，添加输入与输出，接着启动从输入到输出之间的数据流：</p>
<p>Objective-C</p>
<p>AVCaptureSession <em>captureSession = [AVCaptureSession new];<br>AVCaptureDeviceInput </em>cameraDeviceInput = …<br>AVCaptureDeviceInput <em>micDeviceInput = …<br>AVCaptureMovieFileOutput </em>movieFileOutput = … </p>
<p>if ([captureSession canAddInput:cameraDeviceInput]) { </p>
<p>[captureSession addInput:cameraDeviceInput];</p>
<p>}</p>
<p>if ([captureSession canAddInput:micDeviceInput]) { </p>
<p>[captureSession addInput:micDeviceInput];</p>
<p>}</p>
<p>if ([captureSession canAddOutput:movieFileOutput]) { </p>
<p>[captureSession addOutput:movieFileOutput];</p>
<p>}</p>
<p>[captureSession startRunning];<br>(为了简单起见，调度队列 (dispatch queue) 的相关代码已经从上面那段代码中省略了。所有对 capture session 的调用都是阻塞的，因此建议将它们分配到后台串行队列中。)</p>
<p>capture session 可以通过一个 sessionPreset</p>
<p>来进一步配置，这可以用来指定输出质量的等级。有 11 种不同的预设模式：</p>
<p>Objective-C</p>
<p>NSString <em>const  AVCaptureSessionPresetPhoto;<br>NSString </em>const  AVCaptureSessionPresetHigh;<br>NSString <em>const  AVCaptureSessionPresetMedium;<br>NSString </em>const  AVCaptureSessionPresetLow;<br>NSString <em>const  AVCaptureSessionPreset352x288;<br>NSString </em>const  AVCaptureSessionPreset640x480;<br>NSString <em>const  AVCaptureSessionPreset1280x720;<br>NSString </em>const  AVCaptureSessionPreset1920x1080;<br>NSString <em>const  AVCaptureSessionPresetiFrame960x540;<br>NSString </em>const  AVCaptureSessionPresetiFrame1280x720;<br>NSString *const  AVCaptureSessionPresetInputPriority;<br>第一个代表高像素图片输出。 接下来的九个和之前我们在设置 UIImagePickerController 的 videoQuality 时看到过的 UIImagePickerControllerQualityType 选项非常相似，不同的是，这里有一些额外可用于 capture session 的预设。 最后一个 (AVCaptureSessionPresetInputPriority) 代表 capture session 不去控制音频与视频输出设置。而是通过已连接的捕获设备的 activeFormat 来反过来控制 capture session 的输出质量等级。在下一节，我们将会看到更多关于设备和设备格式的细节。</p>
<p>输入<br>AVCaptureSession 的输入其实就是一个或多个的 AVCaptureDevice 对象，这些对象通过 AVCaptureDeviceInput 连接上 capture session。</p>
<p>我们可以使用 [AVCaptureDevice devices] 来寻找可用的捕获设备。以 iPhone 6 为例：</p>
<p>Objective-C</p>
<p>(</p>
<p>“<avcapturefigvideodevice: 0x136514db0="" [back="" camera][com.apple.avfoundation.avcapturedevice.built-in_video:0]="">”,</avcapturefigvideodevice:></p>
<p>“<avcapturefigvideodevice: 0x13660be80="" [front="" camera][com.apple.avfoundation.avcapturedevice.built-in_video:1]="">”,</avcapturefigvideodevice:></p>
<p>“<avcapturefigaudiodevice: 0x174265e80="" [iphone="" microphone][com.apple.avfoundation.avcapturedevice.built-in_audio:0]="">”</avcapturefigaudiodevice:></p>
<p>)<br>视频输入<br>配置相机输入，需要实例化一个 AVCaptureDeviceInput 对象，参数是你期望的相机设备，然后把它添加到 capture session：</p>
<p>Objective-C</p>
<p>AVCaptureSession <em>captureSession = …<br>AVCaptureDevice </em>cameraDevice = [AVCaptureDevice defaultDeviceWithMediaType:AVMediaTypeVideo];<br>NSError *error; </p>
<p>AVCaptureDeviceInput *cameraDeviceInput = [[AVCaptureDeviceInput alloc] initWithDevice:cameraDevice error:&amp;error]; </p>
<p>if ([captureSession canAddInput:input]) {<br>[captureSession addInput:cameraDeviceInput];<br>}<br>如果上面提到的 capture session 预设列表里能满足你的需求，那你就不需要做更多的事情了。如果不够，比如你想要高的帧率，你将需要配置具体的设备格式。一个视频捕获设备有许多设备格式，每个都带有特定的属性和功能。下面是对于 iPhone6 的后置摄像头的一些例子 (一共有 22 种可用格式)：</p>
<p>Objective-C</p>
<p>格式     分辨率        FPS      HRSI       FOV   VIS   最大放大比例  Upscales    AF  ISO SS  HDR</p>
<p>420v    1280x720    5 - 240 1280x720    54.626  YES 49.12   1.09    1   29.0 - 928  0.000003-0.200000   NO</p>
<p>420f    1280x720    5 - 240 1280x720    54.626  YES 49.12   1.09    1   29.0 - 928  0.000003-0.200000   NO</p>
<p>420v    1920x1080   2 - 30  3264x1836   58.040  YES 95.62   1.55    2   29.0 - 464  0.000013-0.500000   YES</p>
<p>420f    1920x1080   2 - 30  3264x1836   58.040  YES 95.62   1.55    2   29.0 - 464  0.000013-0.500000   YES</p>
<p>420v    1920x1080   2 - 60  3264x1836   58.040  YES 95.62   1.55    2   29.0 - 464  0.000008-0.500000   YES</p>
<p>420f    1920x1080   2 - 60  3264x1836   58.040  YES 95.62   1.55    2   29.0 - 464  0.000008-0.500000   YES</p>
<p>格式 = 像素格式<br>FPS = 支持帧数范围<br>HRSI = 高像素静态图片尺寸<br>FOV = 视角<br>VIS = 该格式支持视频防抖<br>Upscales = 加入数字 upscaling 时的放大比例<br>AF = 自动对焦系统（1 是反差对焦，2 是相位对焦）<br>ISO = 支持感光度范围<br>SS = 支持曝光时间范围<br>HDR = 支持高动态范围图像<br>通过上面的那些格式，你会发现如果要录制 240 帧每秒的视频的话，可以根据想要的像素格式选用第一个或第二个格式。另外若是要捕获 1920×1080 的分辨率的视频的话，是不支持 240 帧每秒的。</p>
<p>配置一个具体设备格式，你首先需要调用 lockForConfiguration: 来获取设备的配置属性的独占访问权限。接着你简单地使用 setActiveFormat: 来设置设备的捕获格式。这将会自动把 capture session 的预设设置为 AVCaptureSessionPresetInputPriority。</p>
<p>一旦你设置了预想的设备格式，你就可以在这种设备格式的约束参数范围内进行进一步的配置了。</p>
<p>对于视频捕获的对焦，曝光和白平衡的设置，与图像捕获时一样，具体可参考第 21 期“iOS 上的相机捕捉”。除了那些，这里还有一些视频特有的配置选项。</p>
<p>你可以用捕获设备的 activeVideoMinFrameDuration 和 activeVideoMaxFrameDuration 属性设置帧速率，一帧的时长是帧速率的倒数。设置帧速率之前，要先确认它是否在设备格式所支持的范围内，然后锁住捕获设备来进行配置。为了确保帧速率恒定，可以将最小与最大的帧时长设置成一样的值：</p>
<p>Objective-C</p>
<p>NSError *error; </p>
<p>CMTime frameDuration = CMTimeMake(1, 60);<br>NSArray *supportedFrameRateRanges = [device.activeFormat videoSupportedFrameRateRanges];<br>BOOL frameRateSupported = NO; </p>
<p>for (AVFrameRateRange *range in supportedFrameRateRanges) {<br>if (CMTIME_COMPARE_INLINE(frameDuration, &gt;=, range.minFrameDuration) &amp;&amp;<br>CMTIME_COMPARE_INLINE(frameDuration, &lt;=, range.maxFrameDuration)) {<br>frameRateSupported = YES;<br>}<br>}</p>
<p>if (frameRateSupported &amp;&amp; [device lockForConfiguration:&amp;error]) {<br>[device setActiveVideoMaxFrameDuration:frameDuration];<br>[device setActiveVideoMinFrameDuration:frameDuration];<br>[device unlockForConfiguration];<br>}<br>视频防抖 是在 iOS 6 和 iPhone 4S 发布时引入的功能。到了 iPhone 6，增加了更强劲和流畅的防抖模式，被称为影院级的视频防抖动。相关的 API 也有所改动 (目前为止并没有在文档中反映出来，不过可以查看头文件）。防抖并不是在捕获设备上配置的，而是在 AVCaptureConnection 上设置。由于不是所有的设备格式都支持全部的防抖模式，所以在实际应用中应事先确认具体的防抖模式是否支持：</p>
<p>Objective-C</p>
<p>AVCaptureDevice *device = …; </p>
<p>AVCaptureConnection *connection = …;</p>
<p>AVCaptureVideoStabilizationMode stabilizationMode = AVCaptureVideoStabilizationModeCinematic; </p>
<p>if ([device.activeFormat isVideoStabilizationModeSupported:stabilizationMode]) { </p>
<p>[connection setPreferredVideoStabilizationMode:stabilizationMode];</p>
<p>}<br>iPhone 6 的另一个新特性就是视频 HDR (高动态范围图像)，它是“高动态范围的视频流，与传统的将不同曝光度的静态图像合成成一张高动态范围图像的方法完全不同”，它是内建在传感器中的。有两种方法可以配置视频 HDR：直接将 capture device 的 videoHDREnabled 设置为启用或禁用，或者使用 automaticallyAdjustsVideoHDREnabled 属性来留给系统处理。</p>
<p>技术参考：iPhone 6 和 iPhone Plus 的新 AV Foundation 相机特性</p>
<p>音频输入<br>之前展示的捕获设备列表里面只有一个音频设备，你可能觉得奇怪，毕竟 iPhone 6 有 3 个麦克风。然而因为有时会放在一起使用，便于优化性能，因此可能被当做一个设备来使用。例如在 iPhone 5 及以上的手机录制视频时，会同时使用前置和后置麦克风，用于定向降噪。</p>
<p>Technical Q&amp;A: AVAudioSession – Microphone Selection</p>
<p>大多数情况下，设置成默认的麦克风配置即可。后置麦克风会自动搭配后置摄像头使用 (前置麦克风则用于降噪)，前置麦克风和前置摄像头也是一样。</p>
<p>然而想要访问和配置单独的麦克风也是可行的。例如，当用户正在使用后置摄像头捕获场景的时候，使用前置麦克风来录制解说也应是可能的。这就要依赖于 AVAudioSession。 为了变更要访问的音频，audio session 首先需要设置为支持这样做的类别。然后我们需要遍历 audio session 的输入端口和端口数据来源，来找到我们想要的麦克风：</p>
<p>Objective-C</p>
<p>// 配置 audio session<br>AVAudioSession *audioSession = [AVAudioSession sharedInstance];<br>[audioSession setCategory:AVAudioSessionCategoryPlayAndRecord error:nil];<br>[audioSession setActive:YES error:nil];</p>
<p>// 寻找期望的输入端口<br>NSArray<em> inputs = [audioSession availableInputs];<br>AVAudioSessionPortDescription </em>builtInMic = nil; </p>
<p>for (AVAudioSessionPortDescription* port in inputs) {<br>if ([port.portType isEqualToString:AVAudioSessionPortBuiltInMic]) {<br>builtInMic = port;<br>break;<br>}<br>}</p>
<p>// 寻找期望的麦克风</p>
<p>for (AVAudioSessionDataSourceDescription* source in builtInMic.dataSources) { </p>
<p>if ([source.orientation isEqual:AVAudioSessionOrientationFront]) {<br>[builtInMic setPreferredDataSource:source error:nil];<br>[audioSession setPreferredInput:builtInMic error:&amp;error];<br>break;<br>}<br>}<br>除了设置非默认的麦克风配置，你也可以使用 AVAudioSession 来配置其他音频设置，比如音频增益和采样率等。</p>
<p>访问权限<br>有件事你需要记住，访问相机和麦克风需要先获得用户授权。当你给视频或音频创建第一个 AVCaptureDeviceInput 对象时，iOS 会自动弹出一次对话框，请求用户授权，但你最好还是自己实现下。之后你就可以在还没有被授权的时候，使用相同的代码来提示用户进行授权。当用户未授权时，对于录制视频或音频的尝试，得到的将是黑色画面和无声。</p>
<p>输出<br>输入配置完了，现在把我们的注意力转向 capture session 的输出。</p>
<p>AVCaptureMovieFileOutput</p>
<p>将视频写入文件，最简单的选择就是使用 AVCaptureMovieFileOutput 对象。把它作为输出添加到 capture session 中，就可以将视频和音频写入 QuickTime 文件，这只需很少的配置。</p>
<p>Objective-C</p>
<p>AVCaptureMovieFileOutput *movieFileOutput = [AVCaptureMovieFileOutput new]; </p>
<p>if([captureSession canAddOutput:movieFileOutput]){ </p>
<p>[captureSession addOutput:movieFileOutput];</p>
<p>}</p>
<p>// 开始录制</p>
<p>NSURL *outputURL = … </p>
<p>[movieFileOutput startRecordingToOutputFileURL:outputURL recordingDelegate:self];<br>当实际的录制开始或停止时，想要接收回调的话就必须要一个录制代理。当录制停止时，输出通常还在写入数据，等它完成之后会调用代理方法。</p>
<p>AVCaptureMovieFileOutput 有一些其他的配置选项，比如在某段时间后，在达到某个指定的文件尺寸时，或者当设备的最小磁盘剩余空间达到某个阈值时停止录制。如果你还需要更多设置，比如自定义视频音频的压缩率，或者你想要在写入文件之前，处理视频音频的样本，那么你需要一些更复杂的操作。</p>
<p>AVCaptureDataOutput 和 AVAssetWriter</p>
<p>如果你想要对影音输出有更多的操作，你可以使用 AVCaptureVideoDataOutput 和 AVCaptureAudioDataOutput 而不是我们上节讨论的 AVCaptureMovieFileOutput。</p>
<p>这些输出将会各自捕获视频和音频的样本缓存，接着发送到它们的代理。代理要么对采样缓冲进行处理 (比如给视频加滤镜)，要么保持原样传送。使用 AVAssetWriter 对象可以将样本缓存写入文件：</p>
<p>Using an AVAssetWriter</p>
<p>配置一个 asset writer 需要定义一个输出 URL 和文件格式，并添加一个或多个输入来接收采样的缓冲。我们还需要将输入的 expectsMediaInRealTime属性设置为 YES，因为它们需要从 capture session 实时获得数据。</p>
<p>Objective-C</p>
<p>NSURL *url = …; </p>
<p>AVAssetWriter *assetWriter = [AVAssetWriter assetWriterWithURL:url fileType:AVFileTypeMPEG4 error:nil]; </p>
<p>AVAssetWriterInput *videoInput = [[AVAssetWriterInput alloc] initWithMediaType:AVMediaTypeVideo outputSettings:nil]; </p>
<p>videoInput.expectsMediaDataInRealTime = YES; </p>
<p>AVAssetWriterInput *audioInput = [[AVAssetWriterInput alloc] initWithMediaType:AVMediaTypeAudio outputSettings:nil]; </p>
<p>audioInput.expectsMediaDataInRealTime = YES; </p>
<p>if ([assetWriter canAddInput:videoInput]) { </p>
<p>[assetWriter addInput:videoInput];</p>
<p>}</p>
<p>if ([assetWriter canAddInput:audioInput]) { </p>
<p>[assetWriter addInput:audioInput];</p>
<p>}<br>(这里推荐将 asset writer 派送到后台串行队列中调用。)</p>
<p>在上面的示例代码中，我们将 asset writer 的 outputSettings 设置为 nil。这就意味着附加上来的样本不会再被重新编码。如果你确实想要重新编码这些样本，那么需要提供一个包含具体输出参数的字典。关于音频输出设置的键值被定义在这里, 关于视频输出设置的键值定义在这里。</p>
<p>为了更简单点，AVCaptureVideoDataOutput 和 AVCaptureAudioDataOutput 分别带有 recommendedVideoSettingsForAssetWriterWithOutputFileType: 和 recommendedAudioSettingsForAssetWriterWithOutputFileType: 方法，可以生成与 asset writer 兼容的带有全部键值对的字典。所以你可以通过在这个字典里调整你想要重写的属性，来简单地定义你自己的输出设置。比如，增加视频比特率来提高视频质量等。</p>
<p>或者，你也可以使用 AVOutputSettingsAssistant 来配置输出设置的字典，但是从我的经验来看，使用上面的方法会更好，它们会提供更实用的输出设置，比如视频比特率。另外，AVOutputSettingsAssistant 似乎存在一些缺点，例如，当你改变希望的视频的帧速率时，视频的比特率并不会改变。</p>
<p>实时预览</p>
<p>当使用 AVFoundation 来做图像捕获时，我们必须提供一套自定义的用户界面。其中一个关键的相机交互组件是实时预览图。最简单的实现方式是通过把 AVCaptureVideoPreviewLayer 对象作为一个 sublayer 加到相机图层上去：</p>
<p>Objective-C</p>
<p>AVCaptureSession <em>captureSession = …;<br>AVCaptureVideoPreviewLayer </em>previewLayer = [AVCaptureVideoPreviewLayer layerWithSession:captureSession];<br>UIView *cameraView = …;<br>previewLayer.frame = cameraView.bounds;<br>[cameraView.layer addSublayer:previewLayer];<br>如果你想要更进一步操作，比如，在实时预览图加滤镜，你需要将 AVCaptureVideoDataOutput 对象加到 capture session，并且使用 OpenGL 展示画面，具体可查看该文“iOS 上的相机捕捉”</p>
<p>总结</p>
<p>有许多不同的方法可以给 iOS 上的视频捕获配置管线，从最直接的 UIImagePickerController，到精密配合的 AVCaptureSession 与 AVAssetWriter。如何抉择取决于你的项目要求，比如期望的视频质量和压缩率，或者是你想要展示给用户的相机控件。</p>
<p></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/01/14/iOS三种视屏录制方式/" data-id="cipjtrimz000otjs6a6y118vf" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-iOS-好用的颜色库" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/12/07/iOS-好用的颜色库/" class="article-date">
  <time datetime="2015-12-07T08:38:33.000Z" itemprop="datePublished">2015-12-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/12/07/iOS-好用的颜色库/">iOS_好用的颜色库</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><span></span></p>
<p>Wonderful 这个库主要是与UIColor息息相连的，其中一共包含四个子文件，UIColor+Wonderful，UIColor+Separate，SXColorGradientView，SXMarquee。分别对应颜色扩展，色彩分离，颜色渐变，和跑马灯 4种主要功能。</p>
<p>项目地址：<a href="https://github.com/dsxNiubility/Wonderful" target="_blank" rel="external">https://github.com/dsxNiubility/Wonderful</a></p>
<p>一、UIColor+Wonderful<br>1.这个分类提供了100多种的颜色扩展，譬如卡其色，银色，草坪绿，金色，巧克力色，等等。 虽然这个功能可能不太适合UI规范都直接制定好了的企业级的App，但是对于一些小型的App和Demo类程序还是有一些用武之地的。再也不用redColor buleColor了，再也不用想要一点特殊颜色就要colorWithRGB了。 只要引入了这个库，大部分的常用颜色都可以直接使用。</p>
<p>self.view.backgroundColor = [UIColor khakiColor];<br>self.view.backgroundColor = [UIColor chocolateColor];　<br>2.平均每个色系有10种颜色，不仅可以使用名称直接敲出，还能使用颜色阶梯的宏敲出，在你想不起词的时候更加方便。 宏从1～10是颜色渐深，可以根据自己的感觉使用浅一级的宏或深一级的宏。</p>
<p>self.view.backgroundColor = Wonderful_YelloeColor4;<br>self.view.backgroundColor = Wonderful_BrownColor4;<br>3.提供了颜色微调方案，可以让一个已知颜色的rgb的某值上升或下降若干，可用于不管背景是什么色，边框都比背景深20之类的操作。 也可以将认可颜色的详细值打印出来。</p>
<p>UIColor <em>navColor = [[UIColor redColor]up:SXColorTypeBlue num:30]; // 在红色上把蓝色色值提高30<br>UIColor </em>barColor = [[UIColor blueColor]up:1 num:140]; // 可以直接用枚举对应的tag<br>UIColor <em>bgColor = [[UIColor blackColor]down:SXColorTypeAlpha num:10]; // 取一个比黑色稍微淡的颜色<br>UIColor </em>lineColor = [bgColor up:3 num:20]; // 不管背景是什么颜色 线都比背景浅20.</p>
<p>这个可以先看一下效果展示：（gif图可能比较模糊，但实际上过渡是做的十分高清的）</p>
<p>二、UIColor+Separate<br>1.提供颜色分离方案，可以将任何颜色的RGB 和alpha的值取出。</p>
<p>UIColor <em>testC = [UIColor salmonColor];<br>float r= [testC red];<br>float g= [testC green];<br>float b= [testC blue];<br>float alpha= [testC alpha];<br>NSLog(@”<strong>*</strong></em>  %f,%f,%f,%f”,r,g,b,alpha);<br>2.可以通过一个颜色算出此颜色的反色，使得背景无论被用户设置成什么色，文字颜色都是背景的反色。</p>
<p>self.showLbl1.backgroundColor = [UIColor peachRed];<br>self.showLbl1.textColor = [[UIColor peachRed]reverseColor];<br>3.也可以直接打印这个颜色的各项详细数值</p>
<p>[[UIColor salmonColor]printDetail];<br>// 打印结果<br>This Color’s Red:250, Green:128, Blue:114, Alpha:1<br>decimal red:0.9804 green:0.5020 blue:0.4471<br>Hexadecimal 0xfa8072<br>三、SXColorGradientView<br>1.颜色渐变的view，可以设置任何颜色到透明的过渡。 如果有意，完全可以将导航栏设置成从上往下的渐变色。当下这种渐变色基本都是以一张背景图片，并且还不宜改变，使用原生方法实现渐变色的成本肯定比图片要小。</p>
<p>SXColorGradientView *grv1 = [SXColorGradientView createWithColor:[UIColor paleGreen] frame:CGRectMake(10, 10, 80, 30) visible:YES direction:SXColorGradientToRight];<br>2.也可以设置两个颜色相互过渡。</p>
<p>SXColorGradientView *grv3 = [SXColorGradientView createWithFromColor:[UIColor peruColor] toColor:[UIColor ghostWhite] frame:CGRectMake(10, 50, 80, 30) direction:SXColorGradientToRight];<br>3.可以设置向上下左右四个过渡的方向。</p>
<p>typedef NS_ENUM(NSInteger, SXColorGradientDirection) {<br>SXColorGradientToTop = 1,<br>SXColorGradientToLeft = 2,<br>SXColorGradientToBottom = 3,<br>SXColorGradientToRight = 4,<br>};<br>＊4.后续会增加传入一个数组，然后搭建一个多个颜色过渡的view。</p>
<p>四、SXMarquee<br>1.现在iOS的项目中，带有跑马灯的项目少之又少，安卓有自带的跑马灯控件，但iOS出于各种原因，至少我还没见过带跑马灯的项目，其实这个功能应该是很常用很方便很有意义的。 当下做跑马灯的第三方库没有几个，并且大多存在下列问题：代码老旧，实现复杂，只能实现白色的背景，不能绑定点击事件，点击暂停拖动等。</p>
<p>SXMarquee *mar3 = [[SXMarquee alloc]initWithFrame:CGRectMake(20, 390, 335, 25) speed:2 Msg:@”If you’ve submitted an update to fix a critical bug in your app on the App Store and you are requesting an expedited review.” bgColor:[UIColor goldColor] txtColor:[UIColor goldenrod]];<br>[mar3 changeMarqueeLabelFont:[UIFont boldSystemFontOfSize:12]];<br>[mar3 start];<br>2.跑马灯的背景可以设置任何颜色，这个是基于颜色过渡view做的。</p>
<p>3.跑马灯可以实现点击拖动，或者绑定更多点击事件。</p>
<p></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/12/07/iOS-好用的颜色库/" data-id="cipjtrims000ktjs6enhth5na" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/">__('next') &raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">June 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">April 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">December 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/11/">November 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/10/">October 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/09/">September 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/03/">March 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/02/">February 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/01/">January 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/03/">March 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/02/">February 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/01/">January 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/12/">December 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/11/">November 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/10/">October 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/09/">September 2013</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/06/14/WWDC——2016/">WWDC——2016</a>
          </li>
        
          <li>
            <a href="/2016/04/17/openGL学习/">openGL学习</a>
          </li>
        
          <li>
            <a href="/2016/04/05/MVVM/">MVVM</a>
          </li>
        
          <li>
            <a href="/2016/04/03/MVC/">MVC</a>
          </li>
        
          <li>
            <a href="/2016/03/21/UITableView的应用和其中js与oc交互/">UITableView的应用和其中js与oc交互</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>
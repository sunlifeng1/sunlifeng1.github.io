<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>孙利峰</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="孙利峰">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="孙利峰">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="孙利峰">
<meta name="twitter:description">
  
    <link rel="alternate" href="/atom.xml" title="孙利峰" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">孙利峰</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-搞定iOS与js交互" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/02/15/搞定iOS与js交互/" class="article-date">
  <time datetime="2016-02-15T14:38:11.000Z" itemprop="datePublished">2016-02-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/02/15/搞定iOS与js交互/">搞定iOS与js交互</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><span></span></p>
<p>先看这篇文章： <a href="http://www.open-open.com/lib/view/open1419305655562.html" target="_blank" rel="external">http://www.open-open.com/lib/view/open1419305655562.html</a><br><a href="http://blog.csdn.net/lizhongfu2013/article/details/9232129" target="_blank" rel="external">http://blog.csdn.net/lizhongfu2013/article/details/9232129</a><br><a href="http://blog.csdn.net/lizhongfu2013/article/details/9236357" target="_blank" rel="external">http://blog.csdn.net/lizhongfu2013/article/details/9236357</a><br>干货好文章 <a href="http://www.jianshu.com/p/a329cd4a67ee" target="_blank" rel="external">http://www.jianshu.com/p/a329cd4a67ee</a><br>干货好文章 <a href="http://ios.jobbole.com/84491/" target="_blank" rel="external">http://ios.jobbole.com/84491/</a><br>交互范例 ：<a href="http://www.open-open.com/lib/view/open1463553967635.html" target="_blank" rel="external">http://www.open-open.com/lib/view/open1463553967635.html</a><br><a href="http://www.open-open.com/lib/view/open1456311816823.html" target="_blank" rel="external">http://www.open-open.com/lib/view/open1456311816823.html</a></p>
<p></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/02/15/搞定iOS与js交互/" data-id="cipiddz6o000i113zfo02gju3" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-移动直播开发技术介绍" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/01/28/移动直播开发技术介绍/" class="article-date">
  <time datetime="2016-01-28T14:29:41.000Z" itemprop="datePublished">2016-01-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/01/28/移动直播开发技术介绍/">移动直播开发技术介绍</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><span></span></p>
<p>现今移动直播技术上的挑战要远远难于传统设备或电脑直播，其完整的处理环节包括但不限于：音视频采集、美颜/滤镜/特效处理、编码、封包、推流、转码、分发、解码/渲染/播放等。</p>
<p>直播常见的问题包括</p>
<p>主播在不稳定的网络环境下如何稳定推流？<br>偏远地区的观众如何高清流畅观看直播？<br>直播卡顿时如何智能切换线路？<br>如何精确度量直播质量指标并实时调整？<br>移动设备上不同的芯片平台如何高性能编码和渲染视频？<br>美颜等滤镜特效处理怎么做？<br>如何实现播放秒开？<br>如何保障直播持续播放流畅不卡顿？</p>
<p>本次分享将为大家揭开移动直播核心技术的神秘面纱。</p>
<p>视频、直播等基础知识</p>
<p>什么是视频？</p>
<p>首先我们需要理解一个最基本的概念：视频。从感性的角度来看，视频就是一部充满趣味的影片，可以是电影，可以是短片，是一连贯的视觉冲击力表现丰富的画面和音频。但从理性的角度来看，视频是一种有结构的数据，用工程的语言解释，我们可以把视频剖析成如下结构：</p>
<p>内容元素 ( Content )<br>图像 ( Image )<br>音频 ( Audio )<br>元信息 ( Metadata ) </p>
<p>编码格式 ( Codec )<br>Video : H.264，H.265, …<br>Audio : AAC， HE-AAC, …</p>
<p>容器封装 (Container)<br>MP4，MOV，FLV，RM，RMVB，AVI，…</p>
<p>任何一个视频 Video 文件，从结构上讲，都是这样一种组成方式：</p>
<p>由图像和音频构成最基本的内容元素；<br>图像经过视频编码压缩格式处理（通常是 H.264）；<br>音频经过音频编码压缩格式处理（例如 AAC）；<br>注明相应的元信息（Metadata）；</p>
<p>最后经过一遍容器（Container）封装打包（例如 MP4），构成一个完整的视频文件。</p>
<p>如果觉得难以理解，可以想象成一瓶番茄酱。最外层的瓶子好比这个容器封装（Container），瓶子上注明的原材料和加工厂地等信息好比元信息（Metadata），瓶盖打开（解封装）后，番茄酱本身好比经过压缩处理过后的编码内容，番茄和调料加工成番茄酱的过程就好比编码（Codec），而原材料番茄和调料则好比最原本的内容元素（Content）。</p>
<p>视频的实时传输</p>
<p>简而言之，理性的认知视频的结构后，有助于我们理解视频直播。如果视频是一种“有结构的数据”，那么视频直播无疑是实时传输这种“有结构的数据”（视频）的方式。</p>
<p>那么一个显而易见的问题是：如何实时（Real-Time）传输这种“有结构的数据”（视频）呢？</p>
<p>这里边一个悖论是：一个经过容器（Container）封装后的视频，一定是不可变的 ( Immutable ) 视频文件，不可变的 ( Immutable ) 的视频文件已经是一个生产结果，根据“相对论”，而这个生产结果显然不可能精确到实时的程度，它已经是一段时空的记忆。</p>
<p>因此视频直播，一定是一个 “边生产，边传输，边消费”的过程。这意味着，我们需要更近一步了解视频从原始的内容元素 ( 图像和音频 ) 到成品 ( 视频文件 ) 之前的中间过程 ( 编码 )。</p>
<p>视频编码压缩</p>
<p>不妨让我们来深入浅出理解视频编码压缩技术。</p>
<p>为了便于视频内容的存储和传输，通常需要减少视频内容的体积，也就是需要将原始的内容元素(图像和音频)经过压缩，压缩算法也简称编码格式。例如视频里边的原始图像数据会采用 H.264 编码格式进行压缩，音频采样数据会采用 AAC 编码格式进行压缩。</p>
<p>视频内容经过编码压缩后，确实有利于存储和传输; 不过当要观看播放时，相应地也需要解码过程。因此编码和解码之间，显然需要约定一种编码器和解码器都可以理解的约定。就视频图像编码和解码而言，这种约定很简单：</p>
<p>编码器将多张图像进行编码后生产成一段一段的 GOP ( Group of Pictures ) ， 解码器在播放时则是读取一段一段的 GOP 进行解码后读取画面再渲染显示。</p>
<p>GOP ( Group of Pictures ) 是一组连续的画面，由一张 I 帧和数张 B / P 帧组成，是视频图像编码器和解码器存取的基本单位，它的排列顺序将会一直重复到影像结束。</p>
<p>I 帧是内部编码帧（也称为关键帧），P 帧是前向预测帧（前向参考帧），B 帧是双向内插帧（双向参考帧）。简单地讲，I 帧是一个完整的画面，而 P 帧和 B 帧记录的是相对于 I 帧的变化。</p>
<p>如果没有 I 帧，P 帧和 B 帧就无法解码。</p>
<p>小结一下，一个视频 ( Video ) ，其图像部分的数据是一组 GOP 的集合, 而单个 GOP 则是一组 I / P / B 帧图像的集合。</p>
<p>在这样的一种几何关系中，Video 好比一个 “物体”，GOP 好比 “分子”，I / P / B 帧的图像则好比 “原子”。</p>
<p>想象一下，如果我们把传输一个 “物体”，改成传输一个一个的 “原子”，将最小颗粒以光速传送，那么以人的生物肉眼来感知，将是一种怎样的体验？</p>
<p>什么是视频直播？</p>
<p>不难脑洞大开一下，直播就是这样的一种体验。视频直播技术，就是将视频内容的最小颗粒 ( I / P / B 帧，…)，基于时间序列，以光速进行传送的一种技术。</p>
<p>简而言之，直播就是将每一帧数据 ( Video / Audio / Data Frame )，打上时序标签 ( Timestamp ) 后进行流式传输的过程。发送端源源不断的采集音视频数据，经过编码、封包、推流，再经过中继分发网络进行扩散传播，播放端再源源不断地下载数据并按时序进行解码播放。如此就实现了 “边生产、边传输、边消费” 的直播过程。</p>
<p>理解以上两个关于 视频 和 直播 两个基础概念后，接下来我们就可以一窥直播的业务逻辑了。</p>
<p>直播的业务逻辑</p>
<p>如下是一个最精简的一对多直播业务模型，以及各个层级之间的协议。</p>
<p>各协议差异对比如下</p>
<p>以上就是关于直播技术的一些基础概念。下面我们进一步了解下影响人们视觉体验的直播性能指标。</p>
<p>影响视觉体验的直播性能指标</p>
<p>直播第一个性能指标是延迟，延迟是数据从信息源发送到目的地所需的时间。</p>
<p>根据爱因斯坦的狭义相对论，光速是所有能量、物质和信息运动所能达到的最高速度，这个结论给传播速度设定了上限。因此，即便我们肉眼感觉到的实时，实际上也是有一定的延迟。</p>
<p>由于 RTMP/HLS 是基于 TCP 之上的应用层协议，TCP 三次握手，四次挥手，慢启动过程中的每一次往返来回，都会加上一次往返耗时 ( RTT )，这些交互过程都会增加延迟。</p>
<p>其次根据 TCP 丢包重传特性，网络抖动可能导致丢包重传，也会间接导致延迟加大。</p>
<p>一个完整的直播过程，包括但不限于以下环节：采集、处理、编码、封包、推流、传输、转码、分发、拉流、解码、播放。从推流到播放，再经过中间转发环节，延迟越低，则用户体验越好。</p>
<p>第二个直播性能指标卡顿，是指视频播放过程中出现画面滞帧，让人们明显感觉到“卡”。单位时间内的播放卡顿次数统计称之为卡顿率。</p>
<p>造成卡顿的因素有可能是推流端发送数据中断，也有可能是公网传输拥塞或网络抖动异常，也有可能是终端设备的解码性能太差。卡顿频次越少或没有，则说明用户体验越好。</p>
<p>第三个直播性能指标首屏耗时，指第一次点击播放后，肉眼看到画面所等待的时间。技术上指播放器解码第一帧渲染显示画面所花的耗时。通常说的 “秒开”，指点击播放后，一秒内即可看到播放画面。首屏打开越快，说明用户体验越好。</p>
<p>如上三个直播性能指标，分别对应一个低延迟、高清流畅、极速秒开 的用户体验诉求。了解这三个性能指标，对优化移动直播 APP 的用户体验至关重要。</p>
<p>那么移动直播场景下具体而言有哪些常见的坑呢？</p>
<p>根据实践总结下来的经验，移动平台上视频直播的坑主要可以总结为两方面：设备差异，以及网络环境这些场景下带来的技术考验。</p>
<p>移动直播场景的坑与规避措施</p>
<p>不同芯片平台上的编码差异</p>
<p>iOS 平台上无论硬编还是软编，由于是 Apple 一家公司出厂，几乎不存在因为芯片平台不同而导致的编码差异。</p>
<p>然而，在 Android 平台上，Android Framework SDK 提供的 MediaCodec 编码器，在不同的芯片平台上，差异表现很大， 不同的厂家使用不同的芯片，而不同的芯片平台上 Android MediaCodec 表现略有差异，通常实现全平台兼容的成本不低。</p>
<p>另外就是 Android MediaCodec 硬编层面的 H.264 编码画质参数是固定的 baseline，所以画质通常也一般。因此，在 Android 平台下，推荐是用软编，好处是画质可调控，兼容性也更好。</p>
<p>低端设备如何上高性能地采集和编码？</p>
<p>例如 Camera 采集输出的可能是图片，一张图的体积并不会小，如果采集的频次很高，编码的帧率很高，每张图都经过编码器，那么编码器又可能会出现过载。</p>
<p>这个时候，可以考虑在编码前，不影响画质的前提下（前面我们讲过帧率的微观意义），进行选择性丢帧，以此降低编码环节的功耗开销。</p>
<p>弱网下如何保障高清流畅推流</p>
<p>移动网络下，通常容易遇到网络不稳定，连接被重置，断线重连，一方面频繁重连，建立连接需要开销。另一方面尤其是发生 GPRS / 2G / 3G / 4G 切换时，带宽可能出现瓶颈。当带宽不够，帧率较高/码率较高的内容较难发送出去，这个时候就需要可变码率支持。</p>
<p>即在推流端，可检测网络状态和简单测速，动态来切换码率，以保障网络切换时的推流流畅。</p>
<p>其次编码、封包、推流 这一部分的逻辑也可以做微调，可以尝试选择性丢帧，比如优先丢视频参考帧（不丢 I 帧和音频帧 )，这样也可以减少要传输的数据内容，但同时又达到了不影响画质和版视听流畅的目的。</p>
<p>需要区分直播流的状态和业务状态</p>
<p>直播是媒体流、APP 的交互是 API 信令流，两者的状态不能混为一谈。尤其是不能基于 APP 的交互的 API 状态来判断直播流的状态。</p>
<p>以上是移动直播场景下常见的几个坑和规避措施。</p>
<p>移动直播场景其他优化措施</p>
<p>一、怎么优化打开速度，达到传说中的 “秒开”？</p>
<p>大家可能会看到，市面上某些手机直播 APP 的打开速度非常快，一点就开。而某些手机直播 APP，点击播放后要等好几秒以后才能播放。是什么原因导致如此的天壤之别呢？</p>
<p>大部分播放器都是拿到一个完成的 GOP 后才能解码播放，基于 FFmpeg 移植的播放器甚至需要等待音画时间戳同步后才能播放（如果一个直播里边没有音频只有视频相当于要等待音频超时后才能播放画面）。</p>
<p>“秒开”可以从以下几个方面考虑：</p>
<ol>
<li>改写播放器逻辑让播放器拿到第一个关键帧后就给予显示。</li>
</ol>
<p>GOP 的第一帧通常都是关键帧，由于加载的数据较少，可以达到 “首帧秒开”。</p>
<p>如果直播服务器支持 GOP 缓存，意味着播放器在和服务器建立连接后可立即拿到数据，从而省却跨地域和跨运营商的回源传输时间。</p>
<p>GOP 体现了关键帧的周期，也就是两个关键帧之间的距离，即一个帧组的最大帧数。假设一个视频的恒定帧率是 24fps（即1秒24帧图像），关键帧周期为 2s，那么一个 GOP 就是 48 张图像。一般而言，每一秒视频至少需要使用一个关键帧。</p>
<p>增加关键帧个数可改善画质（GOP 通常为 FPS 的倍数），但是同时增加了带宽和网络负载。这意味着，客户端播放器下载一个 GOP，毕竟该 GOP 存在一定的数据体积，如果播放端网络不佳，有可能不是能够快速在秒级以内下载完该 GOP，进而影响观感体验。</p>
<p>如果不能更改播放器行为逻辑为首帧秒开，直播服务器也可以做一些取巧处理，比如从缓存 GOP 改成缓存双关键帧（减少图像数量），这样可以极大程度地减少播放器加载 GOP 要传输的内容体积。</p>
<ol>
<li>在 APP 业务逻辑层面方面优化。</li>
</ol>
<p>比如提前做好 DNS 解析（省却几十毫秒），和提前做好测速选线（择取最优线路）。经过这样的预处理后，在点击播放按钮时，将极大提高下载性能。</p>
<p>一方面，可以围绕传输层面做性能优化；另一方面，可以围绕客户播放行为做业务逻辑优化。两者可以有效的互为补充，作为秒开的优化空间。</p>
<p>二、美颜等滤镜如何处理？</p>
<p>在手机直播场景下，这就是一个刚需。没有美颜功能的手机直播 APP，主播基本不爱用。可以在采集画面后，将数据送给编码器之前，将数据源回调给滤镜处理程序，原始数据经过滤镜处理完后，再送回给编码器进行编码即可。</p>
<p>除了移动端可以做体验优化之外，直播流媒体服务端架构也可以降低延迟。例如收流服务器主动推送 GOP 至边缘节点，边缘节点缓存 GOP，播放端则可以快速加载，减少回源延迟。</p>
<p>其次，可以贴近终端就近处理和分发</p>
<p>三、如何保障直播持续播放流畅不卡顿？</p>
<p>“秒开”解决的是直播首次加载的播放体验，如何保障直播持续播放过程中的画面和声音视听流畅呢？因为，一个直播毕竟不是一个 HTTP 一样的一次性请求，而是一个 Socket 层面的长连接维持，直到直到主播主动终止推流。</p>
<p>上述我们讲过卡顿的定义：即播放时画面滞帧，触发了人们的视觉感受。在不考虑终端设备性能差异的情况下，针对网络传输层面的原因，我们看看如何保障一个持续的直播不卡顿。</p>
<p>这其实是一个直播过程中传输网络不可靠时的容错问题。例如，播放端临时断网了，但又快速恢复了，针对这种场景，播放端如果不做容错处理，很难不出现黑屏或是重新加载播放的现象。</p>
<p>为了容忍这种网络错误，并达到让终端用户无感知，客户端播放器可以考虑构建一个FIFO（先进先出）的缓冲队列，解码器从播放缓存队列读取数据，缓存队列从直播服务器源源不断的下载数据。通常，缓存队列的容量是以时间为单位（比如3s），在播放端网络不可靠时，客户端缓存区可以起到“断网无感”的过渡作用。</p>
<p>显然，这只是一个“缓兵之计”，如果直播服务器边缘节点出现故障，而此时客户端播放器又是长连接，在无法收到对端的连接断开信号，客户端的缓冲区容量再大也不管用了，这个时候就需要结合客户端业务逻辑来做调度。</p>
<p>重要的是客户端结合服务端，可以做精准调度。在初始化直播推流之前，例如基于 IP 地理位置和运营商的精确调度，分配线路质量最优的边缘接入节点。在直播推流的过程中，可以实时监测帧率反馈等质量数据，基于直播流的质量动态调整线路。</p>
<p>Q &amp; A</p>
<ol>
<li><p>关键帧设置频率一般是多少？有没有根据接入动态设置？过长首屏秒会很难做到。<br>徐立：关键帧间隔越长，也就是 GOP 越长，理论上画面越高清。但是生成 HLS 直播时，最小切割粒度也是一个 GOP，所以针对交互直播，通常不建议 GOP 设置太长。直播一般 2 个关键帧间隔即可。比如帧率是 24fps， 那么 2 个关键帧的间隔就是 48fps ，这个 GOP 就是2s。</p>
</li>
<li><p>七牛这个直播是用的网宿加速？有遇到什么坑没？<br>徐立：七牛在直播方面主要是自建节点，也支持融合众多第三方 CDN 服务商，多样化的线路组合为客户提供更优质的服务。在和第三方 CDN 合作的过程中遇到的问题等有机会再做更细粒度的交流和分享。</p>
</li>
<li><p>RTMP 直播流除了优化线路外，还有什么加速手段吗？<br>徐立：物理上优化线路，逻辑上优化策略，比如选择性丢帧，不影响编码画质的前提下减轻传输体积。</p>
</li>
<li><p>OBS 推流，播放端 HLS 出现视/音频不同步是哪个环节的问题？怎么优化？<br>徐立：有可能是采集端的问题，如果是采集端编码环节就出现音画不同步，可以在收流服务器上做音画时间戳同步，这样是全局的校对。如果是播放端解码性能问题，那么需要调节播放逻辑，比如保证音画时间戳强一致性的前提下，选择性丢一部帧。</p>
</li>
<li><p>PPT 前几页中一个概念好像错了，I 帧不是关键帧，IDR 帧才是。IDR 帧是 I 帧，但是 I 帧不一定是 IDR 帧。只有 IDR 帧才是可重入的。<br>徐立：中文都把 I 帧翻译成关键帧了，不过既然提到了 IDR 帧，可以展开说明一下。所有的 IDR 帧都是 I 帧，但是并不是所有 I 帧都是 IDR 帧，IDR 帧是 I 帧的子集。I 帧严格定义是帧内编码帧，由于是一个全帧压缩编码帧，通常用 I 帧表示 “关键帧”。IDR 是基于 I 帧的一个 “扩展”，带了控制逻辑，IDR 图像都是 I 帧图像，当解码器解码到 IDR 图像时，会立即将参考帧队列清空，将已解码的数据全部输出或抛弃。重新查找参数集，开始一个新的序列。这样如果前一个序列出现重大错误，在这里可以获得重新同步的机会。IDR 图像之后的图像永远不会使用 IDR 之前的图像的数据来解码。</p>
</li>
<li><p>有没有调研过 nginx rtmp module，为什么没有用，对它有什么评价？<br>徐立：有调研过，nginx_rtmp_module 是单进程多线程，非 go 这种轻量级线程/协程用并发自然语义的方式编写流业务。nginx 原本的代码量较大（约 16 万行，但和直播业务相关的功能并不是很多）。且主要靠写 nginx.conf 做配置租户，通常单租户可以，但业务可扩展性方面不是很灵活，可满足基本需求，不满足高级功能。</p>
</li>
<li><p>用到了那些开源软件？编码用的是 x264 吗？直播服务器你们自己开发还是开源的？<br>徐立：直播服务器用 go 开发的，移动端编码优先硬编，软编用 x264</p>
</li>
<li><p>请教一下用 OBS 推流到 nginx_rtmp_module 的时候是已经做了视频压缩了还是需要基于 OBS 再开发？<br>徐立：OBS 把编码压缩都做了，不需要再开发。</p>
</li>
<li><p>视频直播想在 HLS 流中无缝插入一段广告的 ts 文件，有问题想请教一下：1、这段 ts 的分辨率是否一定要和之前的视频流一致？2、pts 时间戳是否要和上一个 ts 递增？<br>徐立：1、可以不一致。这种情况两段视频完全是独立状态，可以没有任何关系，只需要插入 discontinue 标记，播放器在识别到这个标记之后重置解码器参数就可以无缝播放，画面会很平滑的切换。2、不需要递增。举个例子，视频 A 正在直播，播放到 pts 在 5s 的时候，插入一个视频 B，需要先插入一个 discontinue，再插入 B，等 B 播放完之后，再插入一个 discontinue，再插入 A，这个时候 A 的 pts 可以和之前递增，也可以按照中间插入的 B 的时长做偏移，一般做点播和时移的时候 pts 会连续递增，直播的话会算上 B 的时长。</p>
</li>
</ol>
<p>PPT 下载地址</p>
<p><a href="http://77fycs.com2.z0.glb.qiniucdn.com/pili_technology_sharing.pdf" target="_blank" rel="external">http://77fycs.com2.z0.glb.qiniucdn.com/pili_technology_sharing.pdf</a></p>
<p>由于移动直播在实践上还有非常多细节，本文未能全部覆盖，感兴趣的朋友欢迎在文章最后留言讨论。</p>
<p></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/01/28/移动直播开发技术介绍/" data-id="cipiddz7m000m113zfu4usjze" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Bilibili开源的直播框架" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/01/22/Bilibili开源的直播框架/" class="article-date">
  <time datetime="2016-01-22T14:20:09.000Z" itemprop="datePublished">2016-01-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/01/22/Bilibili开源的直播框架/">Bilibili开源的直播框架</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><span></span></p>
<p>文档地址：<a href="http://www.jianshu.com/p/1f06b27b3ac0" target="_blank" rel="external">http://www.jianshu.com/p/1f06b27b3ac0</a></p>
<p>ijkplayer 是一款做视频直播的框架, 基于ffmpeg, 支持 Android 和 iOS, 网上也有很多集成说明, 但是个人觉得还是不够详细, 在这里详细的讲一下在 iOS 中如何集成ijkplayer, 即便以前从没有接触过, 按着下面做也可以集成成功!</p>
<p>一. 下载ijkplayer<br>ijkplayer下载地址:<a href="https://github.com/Bilibili/ijkplayer" target="_blank" rel="external">https://github.com/Bilibili/ijkplayer</a><br>下载完成后解压</p>
<p>二. 编译 ijkplayer<br>说是编译 ijkplayer, 其实是编译 ffmpeg, 在这里我们已经下载好了ijkplayer, 所以 github 上README.md中的Build iOS那一步中有一些步骤是不需要的.<br>下面开始一步一步编译:<br>1.打开终端, cd 到jkplayer-master文件夹中, 也就是下载完解压后的文件夹</p>
<p>2.执行命令行./init-ios.sh, 这一步是去下载 ffmpeg 的, 时间会久一点, 耐心等一下</p>
<p>3.在第2步中下载完成后, 执行cd ios, 也就是进入到 ios目录中</p>
<p>4.进入 ios 文件夹后, 在终端依次执行./compile-ffmpeg.sh clean和./compile-ffmpeg.sh all命令, 编译 ffmpeg, 也就是README.md中这两步</p>
<p>编译 ffmpeg<br>编译时间较久, 耐心等待一下.</p>
<p>三. 打包IJKMediaFramework.framework框架<br>集成 ijkplayer 有两种方法:<br>一种方法是按照IJKMediaDemo工程中那样, 直接导入工程IJKMediaPlayer.xcodeproj, 在这里不做介绍</p>
<p>导入IJKMediaPlayer.xcodeproj<br>第二种集成方法是把 ijkplayer 打包成framework导入工程中使用. 下面开始介绍如何打包IJKMediaFramework.framework, 按下面步骤开始一步一步做:</p>
<p>首先打开工程IJKMediaPlayer.xcodeproj<br>要打包的 framework 工程.png<br>2.工程打开后设置工程的 scheme</p>
<p>3.设置好 scheme 后, 分别选择真机和模拟器进行编译, 编译完成后, 进入 Finder</p>
<p>进入 Finder 后, 可以看到有真机和模拟器两个版本的编译结果</p>
<p>运行后生成的文件.png<br>下面开始合并真机和模拟器版本的 framework, 注意不要合并错了, 合并的是这个文件</p>
<p>合并真机和模拟器文件中的这个文件.png<br>打开终端, 进行合并, 命令行具体格式为:</p>
<p>lipo -create “真机版本路径” “模拟器版本路径” -output “合并后的文件路径”<br>合并</p>
<p>合并生成后的文件.png<br>下面很重要, 需要用合并后的IJKMediaFramework把原来的IJKMediaFramework替换掉, 如下图, 希望你能看懂:</p>
<p>用合并生成的文件替换原来的文件.png<br>上图中的1、2两步完成后, 绿色框住的那个IJKMediaFramework.framework文件就是我们需要的框架了, 可以复制出来, 稍后我们需要导入工程使用.</p>
<p>四. iOS工程中集成ijkplayer<br>新建工程, 导入合并后的IJKMediaFramework.framework以及相关依赖框架以及相关依赖框架</p>
<p>导入 framework及依赖框架.png<br>导入框架后, 在ViewController.m进行测试, 首先导入IJKMediaFramework.h头文件, 编译看有没有错, 如果没有错说明集成成功.<br>接着开始在ViewController.m文件中使用IJKMediaFramework框架进行测试使用, 写一个简单的直播视频进行测试, 在这里看一下运行后的结果, 后面会放上 Demo 供下载.</p>
<p>文／jianshu_wl（简书作者）<br>原文链接：<a href="http://www.jianshu.com/p/1f06b27b3ac0" target="_blank" rel="external">http://www.jianshu.com/p/1f06b27b3ac0</a><br>著作权归作者所有，转载请联系作者获得授权，并标注“简书作者”。</p>
<p></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/01/22/Bilibili开源的直播框架/" data-id="cipiddz4p0002113zlu2o4iag" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-iOS三种视屏录制方式" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/01/14/iOS三种视屏录制方式/" class="article-date">
  <time datetime="2016-01-14T14:14:49.000Z" itemprop="datePublished">2016-01-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/01/14/iOS三种视屏录制方式/">iOS三种视屏录制方式</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><span></span></p>
<p>随着每一代 iPhone 处理能力和相机硬件配置的提高，使用它来捕获视频也变得更加有意思。它们小巧，轻便，低调，而且与专业摄像机之间的差距已经变得非常小，小到在某些情况下，iPhone 可以真正替代它们。</p>
<p>这篇文章讨论了关于如何配置视频捕获管线 (pipeline) 和最大限度地利用硬件性能的一些不同选择。 这里有个使用了不同管线的样例 app，可以在 GitHub 查看。</p>
<p>UIImagePickerController</p>
<p>目前，将视频捕获集成到你的应用中的最简单的方法是使用 UIImagePickerController。这是一个封装了完整视频捕获管线和相机 UI 的 view controller。</p>
<p>在实例化相机之前，首先要检查设备是否支持相机录制：</p>
<p>Objective-C</p>
<p>if ([UIImagePickerController<br>isSourceTypeAvailable:UIImagePickerControllerSourceTypeCamera]) {<br>NSArray <em>availableMediaTypes = [UIImagePickerController<br>availableMediaTypesForSourceType:UIImagePickerControllerSourceTypeCamera];<br>if ([availableMediaTypes containsObject:(NSString </em>)kUTTypeMovie]) {<br>// 支持视频录制<br>}<br>}<br>然后创建一个 UIImagePickerController 对象，设置好代理便于进一步处理录制好的视频 (比如存到相册) 以及对于用户关闭相机作出响应：</p>
<p>Objective-C</p>
<p>UIImagePickerController <em>camera = [UIImagePickerController new];<br>camera.sourceType = UIImagePickerControllerSourceTypeCamera;<br>camera.mediaTypes = @[(NSString </em>)kUTTypeMovie];<br>camera.delegate = self;<br>这是你实现一个功能完善的摄像机所需要写的所有代码。</p>
<p>相机配置</p>
<p>UIImagePickerController 提供了额外的配置选项。</p>
<p>通过设置 cameraDevice 属性可以选择一个特定的相机。这是一个 UIImagePickerControllerCameraDevice 枚举，默认情况下是 UIImagePickerControllerCameraDeviceRear，你也可以把它设置为 UIImagePickerControllerCameraDeviceFront。每次都应事先确认你想要设置的相机是可用的：</p>
<p>Objective-C</p>
<p>UIImagePickerController *camera = …<br>if ([UIImagePickerController isCameraDeviceAvailable:UIImagePickerControllerCameraDeviceFront]) {<br>[camera setCameraDevice:UIImagePickerControllerCameraDeviceFront];<br>}<br>videoQuality 属性用于控制录制视频的质量。它允许你设置一个特定的编码预设，从而改变视频的比特率和分辨率。以下是六种预设：</p>
<p>Objective-C</p>
<p>enum {<br>UIImagePickerControllerQualityTypeHigh             = 0,<br>UIImagePickerControllerQualityTypeMedium           = 1,  // default  value<br>UIImagePickerControllerQualityTypeLow              = 2,<br>UIImagePickerControllerQualityType640x480          = 3,<br>UIImagePickerControllerQualityTypeIFrame1280x720   = 4,<br>UIImagePickerControllerQualityTypeIFrame960x540    = 5<br>};<br>typedef NSUInteger  UIImagePickerControllerQualityType;<br>前三种为相对预设 (low, medium, high)。这些预设的编码配置会因设备不同而不同。如果选择 high，那么你选定的相机会提供给你该设备所能支持的最高画质。后面三种是特定分辨率的预设 (640×480 VGA, 960×540 iFrame, 和 1280×720 iFrame)。</p>
<p>自定义 UI</p>
<p>就像上面提到的，UIImagePickerController 自带一套相机 UI，可以直接使用。然而，你也可以自定义相机的控件，通过隐藏默认控件，然后创建带有控件的自定义视图，并覆盖在相机预览图层上面：</p>
<p>Objective-C</p>
<p>UIView *cameraOverlay = …<br>picker.showsCameraControls = NO;<br>picker.cameraOverlayView = cameraOverlay;<br>然后你需要将你覆盖层上的控件关联上 UIImagePickerController 的控制方法 (比如，startVideoCapture 和 stopVideoCapture)。</p>
<p>AVFoundation</p>
<p>如果你想要更多关于处理捕获视频的方法，而这些方法是 UIImagePickerController 所不能提供的，那么你需要使用 AVFoundation。</p>
<p>AVFoundation 中关于视频捕获的主要的类是 AVCaptureSession。它负责调配影音输入与输出之间的数据流：</p>
<p>AVCaptureSession setup<br>使用一个 capture session，你需要先实例化，添加输入与输出，接着启动从输入到输出之间的数据流：</p>
<p>Objective-C</p>
<p>AVCaptureSession <em>captureSession = [AVCaptureSession new];<br>AVCaptureDeviceInput </em>cameraDeviceInput = …<br>AVCaptureDeviceInput <em>micDeviceInput = …<br>AVCaptureMovieFileOutput </em>movieFileOutput = … </p>
<p>if ([captureSession canAddInput:cameraDeviceInput]) { </p>
<p>[captureSession addInput:cameraDeviceInput];</p>
<p>}</p>
<p>if ([captureSession canAddInput:micDeviceInput]) { </p>
<p>[captureSession addInput:micDeviceInput];</p>
<p>}</p>
<p>if ([captureSession canAddOutput:movieFileOutput]) { </p>
<p>[captureSession addOutput:movieFileOutput];</p>
<p>}</p>
<p>[captureSession startRunning];<br>(为了简单起见，调度队列 (dispatch queue) 的相关代码已经从上面那段代码中省略了。所有对 capture session 的调用都是阻塞的，因此建议将它们分配到后台串行队列中。)</p>
<p>capture session 可以通过一个 sessionPreset</p>
<p>来进一步配置，这可以用来指定输出质量的等级。有 11 种不同的预设模式：</p>
<p>Objective-C</p>
<p>NSString <em>const  AVCaptureSessionPresetPhoto;<br>NSString </em>const  AVCaptureSessionPresetHigh;<br>NSString <em>const  AVCaptureSessionPresetMedium;<br>NSString </em>const  AVCaptureSessionPresetLow;<br>NSString <em>const  AVCaptureSessionPreset352x288;<br>NSString </em>const  AVCaptureSessionPreset640x480;<br>NSString <em>const  AVCaptureSessionPreset1280x720;<br>NSString </em>const  AVCaptureSessionPreset1920x1080;<br>NSString <em>const  AVCaptureSessionPresetiFrame960x540;<br>NSString </em>const  AVCaptureSessionPresetiFrame1280x720;<br>NSString *const  AVCaptureSessionPresetInputPriority;<br>第一个代表高像素图片输出。 接下来的九个和之前我们在设置 UIImagePickerController 的 videoQuality 时看到过的 UIImagePickerControllerQualityType 选项非常相似，不同的是，这里有一些额外可用于 capture session 的预设。 最后一个 (AVCaptureSessionPresetInputPriority) 代表 capture session 不去控制音频与视频输出设置。而是通过已连接的捕获设备的 activeFormat 来反过来控制 capture session 的输出质量等级。在下一节，我们将会看到更多关于设备和设备格式的细节。</p>
<p>输入<br>AVCaptureSession 的输入其实就是一个或多个的 AVCaptureDevice 对象，这些对象通过 AVCaptureDeviceInput 连接上 capture session。</p>
<p>我们可以使用 [AVCaptureDevice devices] 来寻找可用的捕获设备。以 iPhone 6 为例：</p>
<p>Objective-C</p>
<p>(</p>
<p>“<avcapturefigvideodevice: 0x136514db0="" [back="" camera][com.apple.avfoundation.avcapturedevice.built-in_video:0]="">”,</avcapturefigvideodevice:></p>
<p>“<avcapturefigvideodevice: 0x13660be80="" [front="" camera][com.apple.avfoundation.avcapturedevice.built-in_video:1]="">”,</avcapturefigvideodevice:></p>
<p>“<avcapturefigaudiodevice: 0x174265e80="" [iphone="" microphone][com.apple.avfoundation.avcapturedevice.built-in_audio:0]="">”</avcapturefigaudiodevice:></p>
<p>)<br>视频输入<br>配置相机输入，需要实例化一个 AVCaptureDeviceInput 对象，参数是你期望的相机设备，然后把它添加到 capture session：</p>
<p>Objective-C</p>
<p>AVCaptureSession <em>captureSession = …<br>AVCaptureDevice </em>cameraDevice = [AVCaptureDevice defaultDeviceWithMediaType:AVMediaTypeVideo];<br>NSError *error; </p>
<p>AVCaptureDeviceInput *cameraDeviceInput = [[AVCaptureDeviceInput alloc] initWithDevice:cameraDevice error:&amp;error]; </p>
<p>if ([captureSession canAddInput:input]) {<br>[captureSession addInput:cameraDeviceInput];<br>}<br>如果上面提到的 capture session 预设列表里能满足你的需求，那你就不需要做更多的事情了。如果不够，比如你想要高的帧率，你将需要配置具体的设备格式。一个视频捕获设备有许多设备格式，每个都带有特定的属性和功能。下面是对于 iPhone6 的后置摄像头的一些例子 (一共有 22 种可用格式)：</p>
<p>Objective-C</p>
<p>格式     分辨率        FPS      HRSI       FOV   VIS   最大放大比例  Upscales    AF  ISO SS  HDR</p>
<p>420v    1280x720    5 - 240 1280x720    54.626  YES 49.12   1.09    1   29.0 - 928  0.000003-0.200000   NO</p>
<p>420f    1280x720    5 - 240 1280x720    54.626  YES 49.12   1.09    1   29.0 - 928  0.000003-0.200000   NO</p>
<p>420v    1920x1080   2 - 30  3264x1836   58.040  YES 95.62   1.55    2   29.0 - 464  0.000013-0.500000   YES</p>
<p>420f    1920x1080   2 - 30  3264x1836   58.040  YES 95.62   1.55    2   29.0 - 464  0.000013-0.500000   YES</p>
<p>420v    1920x1080   2 - 60  3264x1836   58.040  YES 95.62   1.55    2   29.0 - 464  0.000008-0.500000   YES</p>
<p>420f    1920x1080   2 - 60  3264x1836   58.040  YES 95.62   1.55    2   29.0 - 464  0.000008-0.500000   YES</p>
<p>格式 = 像素格式<br>FPS = 支持帧数范围<br>HRSI = 高像素静态图片尺寸<br>FOV = 视角<br>VIS = 该格式支持视频防抖<br>Upscales = 加入数字 upscaling 时的放大比例<br>AF = 自动对焦系统（1 是反差对焦，2 是相位对焦）<br>ISO = 支持感光度范围<br>SS = 支持曝光时间范围<br>HDR = 支持高动态范围图像<br>通过上面的那些格式，你会发现如果要录制 240 帧每秒的视频的话，可以根据想要的像素格式选用第一个或第二个格式。另外若是要捕获 1920×1080 的分辨率的视频的话，是不支持 240 帧每秒的。</p>
<p>配置一个具体设备格式，你首先需要调用 lockForConfiguration: 来获取设备的配置属性的独占访问权限。接着你简单地使用 setActiveFormat: 来设置设备的捕获格式。这将会自动把 capture session 的预设设置为 AVCaptureSessionPresetInputPriority。</p>
<p>一旦你设置了预想的设备格式，你就可以在这种设备格式的约束参数范围内进行进一步的配置了。</p>
<p>对于视频捕获的对焦，曝光和白平衡的设置，与图像捕获时一样，具体可参考第 21 期“iOS 上的相机捕捉”。除了那些，这里还有一些视频特有的配置选项。</p>
<p>你可以用捕获设备的 activeVideoMinFrameDuration 和 activeVideoMaxFrameDuration 属性设置帧速率，一帧的时长是帧速率的倒数。设置帧速率之前，要先确认它是否在设备格式所支持的范围内，然后锁住捕获设备来进行配置。为了确保帧速率恒定，可以将最小与最大的帧时长设置成一样的值：</p>
<p>Objective-C</p>
<p>NSError *error; </p>
<p>CMTime frameDuration = CMTimeMake(1, 60);<br>NSArray *supportedFrameRateRanges = [device.activeFormat videoSupportedFrameRateRanges];<br>BOOL frameRateSupported = NO; </p>
<p>for (AVFrameRateRange *range in supportedFrameRateRanges) {<br>if (CMTIME_COMPARE_INLINE(frameDuration, &gt;=, range.minFrameDuration) &amp;&amp;<br>CMTIME_COMPARE_INLINE(frameDuration, &lt;=, range.maxFrameDuration)) {<br>frameRateSupported = YES;<br>}<br>}</p>
<p>if (frameRateSupported &amp;&amp; [device lockForConfiguration:&amp;error]) {<br>[device setActiveVideoMaxFrameDuration:frameDuration];<br>[device setActiveVideoMinFrameDuration:frameDuration];<br>[device unlockForConfiguration];<br>}<br>视频防抖 是在 iOS 6 和 iPhone 4S 发布时引入的功能。到了 iPhone 6，增加了更强劲和流畅的防抖模式，被称为影院级的视频防抖动。相关的 API 也有所改动 (目前为止并没有在文档中反映出来，不过可以查看头文件）。防抖并不是在捕获设备上配置的，而是在 AVCaptureConnection 上设置。由于不是所有的设备格式都支持全部的防抖模式，所以在实际应用中应事先确认具体的防抖模式是否支持：</p>
<p>Objective-C</p>
<p>AVCaptureDevice *device = …; </p>
<p>AVCaptureConnection *connection = …;</p>
<p>AVCaptureVideoStabilizationMode stabilizationMode = AVCaptureVideoStabilizationModeCinematic; </p>
<p>if ([device.activeFormat isVideoStabilizationModeSupported:stabilizationMode]) { </p>
<p>[connection setPreferredVideoStabilizationMode:stabilizationMode];</p>
<p>}<br>iPhone 6 的另一个新特性就是视频 HDR (高动态范围图像)，它是“高动态范围的视频流，与传统的将不同曝光度的静态图像合成成一张高动态范围图像的方法完全不同”，它是内建在传感器中的。有两种方法可以配置视频 HDR：直接将 capture device 的 videoHDREnabled 设置为启用或禁用，或者使用 automaticallyAdjustsVideoHDREnabled 属性来留给系统处理。</p>
<p>技术参考：iPhone 6 和 iPhone Plus 的新 AV Foundation 相机特性</p>
<p>音频输入<br>之前展示的捕获设备列表里面只有一个音频设备，你可能觉得奇怪，毕竟 iPhone 6 有 3 个麦克风。然而因为有时会放在一起使用，便于优化性能，因此可能被当做一个设备来使用。例如在 iPhone 5 及以上的手机录制视频时，会同时使用前置和后置麦克风，用于定向降噪。</p>
<p>Technical Q&amp;A: AVAudioSession – Microphone Selection</p>
<p>大多数情况下，设置成默认的麦克风配置即可。后置麦克风会自动搭配后置摄像头使用 (前置麦克风则用于降噪)，前置麦克风和前置摄像头也是一样。</p>
<p>然而想要访问和配置单独的麦克风也是可行的。例如，当用户正在使用后置摄像头捕获场景的时候，使用前置麦克风来录制解说也应是可能的。这就要依赖于 AVAudioSession。 为了变更要访问的音频，audio session 首先需要设置为支持这样做的类别。然后我们需要遍历 audio session 的输入端口和端口数据来源，来找到我们想要的麦克风：</p>
<p>Objective-C</p>
<p>// 配置 audio session<br>AVAudioSession *audioSession = [AVAudioSession sharedInstance];<br>[audioSession setCategory:AVAudioSessionCategoryPlayAndRecord error:nil];<br>[audioSession setActive:YES error:nil];</p>
<p>// 寻找期望的输入端口<br>NSArray<em> inputs = [audioSession availableInputs];<br>AVAudioSessionPortDescription </em>builtInMic = nil; </p>
<p>for (AVAudioSessionPortDescription* port in inputs) {<br>if ([port.portType isEqualToString:AVAudioSessionPortBuiltInMic]) {<br>builtInMic = port;<br>break;<br>}<br>}</p>
<p>// 寻找期望的麦克风</p>
<p>for (AVAudioSessionDataSourceDescription* source in builtInMic.dataSources) { </p>
<p>if ([source.orientation isEqual:AVAudioSessionOrientationFront]) {<br>[builtInMic setPreferredDataSource:source error:nil];<br>[audioSession setPreferredInput:builtInMic error:&amp;error];<br>break;<br>}<br>}<br>除了设置非默认的麦克风配置，你也可以使用 AVAudioSession 来配置其他音频设置，比如音频增益和采样率等。</p>
<p>访问权限<br>有件事你需要记住，访问相机和麦克风需要先获得用户授权。当你给视频或音频创建第一个 AVCaptureDeviceInput 对象时，iOS 会自动弹出一次对话框，请求用户授权，但你最好还是自己实现下。之后你就可以在还没有被授权的时候，使用相同的代码来提示用户进行授权。当用户未授权时，对于录制视频或音频的尝试，得到的将是黑色画面和无声。</p>
<p>输出<br>输入配置完了，现在把我们的注意力转向 capture session 的输出。</p>
<p>AVCaptureMovieFileOutput</p>
<p>将视频写入文件，最简单的选择就是使用 AVCaptureMovieFileOutput 对象。把它作为输出添加到 capture session 中，就可以将视频和音频写入 QuickTime 文件，这只需很少的配置。</p>
<p>Objective-C</p>
<p>AVCaptureMovieFileOutput *movieFileOutput = [AVCaptureMovieFileOutput new]; </p>
<p>if([captureSession canAddOutput:movieFileOutput]){ </p>
<p>[captureSession addOutput:movieFileOutput];</p>
<p>}</p>
<p>// 开始录制</p>
<p>NSURL *outputURL = … </p>
<p>[movieFileOutput startRecordingToOutputFileURL:outputURL recordingDelegate:self];<br>当实际的录制开始或停止时，想要接收回调的话就必须要一个录制代理。当录制停止时，输出通常还在写入数据，等它完成之后会调用代理方法。</p>
<p>AVCaptureMovieFileOutput 有一些其他的配置选项，比如在某段时间后，在达到某个指定的文件尺寸时，或者当设备的最小磁盘剩余空间达到某个阈值时停止录制。如果你还需要更多设置，比如自定义视频音频的压缩率，或者你想要在写入文件之前，处理视频音频的样本，那么你需要一些更复杂的操作。</p>
<p>AVCaptureDataOutput 和 AVAssetWriter</p>
<p>如果你想要对影音输出有更多的操作，你可以使用 AVCaptureVideoDataOutput 和 AVCaptureAudioDataOutput 而不是我们上节讨论的 AVCaptureMovieFileOutput。</p>
<p>这些输出将会各自捕获视频和音频的样本缓存，接着发送到它们的代理。代理要么对采样缓冲进行处理 (比如给视频加滤镜)，要么保持原样传送。使用 AVAssetWriter 对象可以将样本缓存写入文件：</p>
<p>Using an AVAssetWriter</p>
<p>配置一个 asset writer 需要定义一个输出 URL 和文件格式，并添加一个或多个输入来接收采样的缓冲。我们还需要将输入的 expectsMediaInRealTime属性设置为 YES，因为它们需要从 capture session 实时获得数据。</p>
<p>Objective-C</p>
<p>NSURL *url = …; </p>
<p>AVAssetWriter *assetWriter = [AVAssetWriter assetWriterWithURL:url fileType:AVFileTypeMPEG4 error:nil]; </p>
<p>AVAssetWriterInput *videoInput = [[AVAssetWriterInput alloc] initWithMediaType:AVMediaTypeVideo outputSettings:nil]; </p>
<p>videoInput.expectsMediaDataInRealTime = YES; </p>
<p>AVAssetWriterInput *audioInput = [[AVAssetWriterInput alloc] initWithMediaType:AVMediaTypeAudio outputSettings:nil]; </p>
<p>audioInput.expectsMediaDataInRealTime = YES; </p>
<p>if ([assetWriter canAddInput:videoInput]) { </p>
<p>[assetWriter addInput:videoInput];</p>
<p>}</p>
<p>if ([assetWriter canAddInput:audioInput]) { </p>
<p>[assetWriter addInput:audioInput];</p>
<p>}<br>(这里推荐将 asset writer 派送到后台串行队列中调用。)</p>
<p>在上面的示例代码中，我们将 asset writer 的 outputSettings 设置为 nil。这就意味着附加上来的样本不会再被重新编码。如果你确实想要重新编码这些样本，那么需要提供一个包含具体输出参数的字典。关于音频输出设置的键值被定义在这里, 关于视频输出设置的键值定义在这里。</p>
<p>为了更简单点，AVCaptureVideoDataOutput 和 AVCaptureAudioDataOutput 分别带有 recommendedVideoSettingsForAssetWriterWithOutputFileType: 和 recommendedAudioSettingsForAssetWriterWithOutputFileType: 方法，可以生成与 asset writer 兼容的带有全部键值对的字典。所以你可以通过在这个字典里调整你想要重写的属性，来简单地定义你自己的输出设置。比如，增加视频比特率来提高视频质量等。</p>
<p>或者，你也可以使用 AVOutputSettingsAssistant 来配置输出设置的字典，但是从我的经验来看，使用上面的方法会更好，它们会提供更实用的输出设置，比如视频比特率。另外，AVOutputSettingsAssistant 似乎存在一些缺点，例如，当你改变希望的视频的帧速率时，视频的比特率并不会改变。</p>
<p>实时预览</p>
<p>当使用 AVFoundation 来做图像捕获时，我们必须提供一套自定义的用户界面。其中一个关键的相机交互组件是实时预览图。最简单的实现方式是通过把 AVCaptureVideoPreviewLayer 对象作为一个 sublayer 加到相机图层上去：</p>
<p>Objective-C</p>
<p>AVCaptureSession <em>captureSession = …;<br>AVCaptureVideoPreviewLayer </em>previewLayer = [AVCaptureVideoPreviewLayer layerWithSession:captureSession];<br>UIView *cameraView = …;<br>previewLayer.frame = cameraView.bounds;<br>[cameraView.layer addSublayer:previewLayer];<br>如果你想要更进一步操作，比如，在实时预览图加滤镜，你需要将 AVCaptureVideoDataOutput 对象加到 capture session，并且使用 OpenGL 展示画面，具体可查看该文“iOS 上的相机捕捉”</p>
<p>总结</p>
<p>有许多不同的方法可以给 iOS 上的视频捕获配置管线，从最直接的 UIImagePickerController，到精密配合的 AVCaptureSession 与 AVAssetWriter。如何抉择取决于你的项目要求，比如期望的视频质量和压缩率，或者是你想要展示给用户的相机控件。</p>
<p></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/01/14/iOS三种视屏录制方式/" data-id="cipiddz5i0009113z0kwcsdw6" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-iOS-JSON解析四种方法" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/01/07/iOS-JSON解析四种方法/" class="article-date">
  <time datetime="2015-01-07T13:52:08.000Z" itemprop="datePublished">2015-01-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/01/07/iOS-JSON解析四种方法/">iOS_JSON解析四种方法</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><span></span></p>
<p>相关链接：<a href="http://blog.csdn.net/enuola/article/details/7903632" target="_blank" rel="external">http://blog.csdn.net/enuola/article/details/7903632</a></p>
<p>从IOS5开始，APPLE提供了对json的原生支持（NSJSONSerialization），但是为了兼容以前的iOS版本，可以使用第三方库来解析Json。</p>
<p>本文将介绍TouchJson、 SBJson 、JSONKit 和 iOS5所支持的原生的json方法，解析国家气象局API，TouchJson和SBJson需要下载他们的库</p>
<p>TouchJson包下载： <a href="http://download.csdn.net/detail/enuola/4523169" target="_blank" rel="external">http://download.csdn.net/detail/enuola/4523169</a></p>
<p>SBJson 包下载： <a href="http://download.csdn.net/detail/enuola/4523177" target="_blank" rel="external">http://download.csdn.net/detail/enuola/4523177</a></p>
<p>JSONKit包下载：<a href="http://download.csdn.net/detail/enuola/4523160" target="_blank" rel="external">http://download.csdn.net/detail/enuola/4523160</a></p>
<p>下面的完整程序源码包下载：<a href="http://download.csdn.net/detail/enuola/4523223" target="_blank" rel="external">http://download.csdn.net/detail/enuola/4523223</a></p>
<p></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2015/01/07/iOS-JSON解析四种方法/" data-id="cipiddz510005113zlnvztv8l" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-iOS中使用ZXing库" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2014/01/20/iOS中使用ZXing库/" class="article-date">
  <time datetime="2014-01-20T14:02:30.000Z" itemprop="datePublished">2014-01-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/01/20/iOS中使用ZXing库/">iOS中使用ZXing库</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><span></span></p>
<p>前言<br>ZXing(Github镜像地址)是一个开源的条码生成和扫描库（开源协议为Apache2.0)。它不但支持众多的条码格式，而且有各种语言的实现版本，它支持的语言包括：Java, C++, C#, Objective-C, ActionScript和Ruby。<br>我上周在iOS项目开发中使用了ZXing的扫描二维码功能。在此总结一下如何将ZXing集成到已有的iOS工程中，分享给大家。<br>集成步骤<br>首先去Google Code或Github将ZXing的代码下载下来，整个工程比较大，我们只需要其中涉及iOS的部分，所以最好做一些裁剪。简单来说，我们只需要保留cpp和iphone这2个文件夹，其余的全部删掉。如下图所示：</p>
<p>接着我们继续裁剪，对于cpp这个目录，只保留cpp/core/src/zxing下面的内容，其余内容也可以删掉了。但是整个目录结构必须保持原样。裁剪完后，整个目录结构如下所示：</p>
<p>接下来，我们把裁剪后的zxing目录整个移动到我们的iOS项目的目录下，并且把上图中可以看到的ZXingWidget.xcodeproj文件拖动到我们的iOS工程中。<br>下一步，我们需要设置ZXing项目和我们原本的iOS项目之间的依赖关系。在我们的iOS项目的设置中，点击build phases tab，然后增加 Target Dependencies 和 Link binary，并且增加这些framework依赖：<br>a. AVFoundation<br>b. AudioToolbox<br>c. CoreVideo<br>d. CoreMedia<br>e. libiconv<br>f. AddressBook<br>g. AddressBookUI</p>
<p>最后一步，在设置中增加如下2个header search path:<br>./zxing/iphone/ZXingWidget/Classes<br>./zxing/cpp/core/src<br>需要注意的是，第一个path要设置成循环查找子目录，而第二个不循环查找，如下图所示：</p>
<p>恭喜你，完成这步之后，你就已经完成ZXing库的集成了。下面谈谈如何使用ZXing库来做二维码识别。<br>二维码识别<br>ZXing的iOS版本提供2种方法来做二维码识别功能，第一种方法比较简单，第二种方法比较复杂。我在做Demo时使用了第一种方法，做真正项目开发的时候使用了第二种方法，所以都给大家介绍一下。<br>使用方法一<br>ZXing直接提供了一个扫描二维码的View Controller，即ZXingWidgetController。在需要使用的界面代码中，加入文件依赖：</p>
<p>#import <zxingwidgetcontroller.h></zxingwidgetcontroller.h></p>
<p>#import <qrcodereader.h><br><code>然后在需要扫描的时候，调用如下代码即可：</code> objc</qrcodereader.h></p>
<ul>
<li>(IBAction)scanPressed:(id)sender {<br>ZXingWidgetController <em>widController = [[ZXingWidgetController alloc] initWithDelegate:self showCancel:YES OneDMode:NO];<br>NSMutableSet </em>readers = [[NSMutableSet alloc ] init];<br>QRCodeReader* qrcodeReader = [[QRCodeReader alloc] init];<br>[readers addObject:qrcodeReader];<br>[qrcodeReader release];<br>widController.readers = readers;<br>[readers release];<br>[self presentModalViewController:widController animated:YES];<br>[widController release];<br>}<br>在ZXing扫描有结果时，会调用如下回调函数：</li>
</ul>
<p>@protocol ZXingDelegate</p>
<ul>
<li>(void)zxingController:(ZXingWidgetController<em>)controller didScanResult:(NSString </em>)result;</li>
<li>(void)zxingControllerDidCancel:(ZXingWidgetController*)controller;<br>@end<br>使用方法二<br>方法二与方法一的区别就相当于AVFoundation和UIImagePickerController的区别一样。简单来说，就是使用方法二比方法一更加麻烦，但是获得的可定制性更高。<br>使用方法二时，你需要自己用AVFoundation获得Camera返回的实时图象，然后转成UIImage，最后传给ZXing的Decoder类完成二维码的识别。由于使用AVFoundation涉及的代码略多，我写的示意代码如下：</li>
</ul>
<p>#import “Decoder.h”</p>
<p>#import “TwoDDecoderResult.h”</p>
<p>#import “QRCodeReader.h”</p>
<ul>
<li>(void)viewDidLoad {<br>// setup QR reader<br>self.qrReader = [[NSMutableSet alloc ] init];<br>QRCodeReader* qrcodeReader = [[QRCodeReader alloc] init];<br>[self.qrReader addObject:qrcodeReader];<br>self.scanningQR = NO;<br>self.step = STEP_QR;<br>}</li>
</ul>
<p>// AVFoundation的回调函数</p>
<ul>
<li>(void)captureOutput:(AVCaptureOutput <em>)captureOutput didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection </em>)connection {<br>// 第一步，将sampleBuffer转成UIImage<br>UIImage <em>image= [self getCaptureImage:sampleBuffer];<br>// 第二步，用Decoder识别图象<br>Decoder </em>d = [[Decoder alloc] init];<br>d.readers = self.qrReader;<br>d.delegate = self;<br>self.scanningQR = [d decodeImage:image] == YES ? NO : YES;<br>}<br>ZXing的Decoder类提供了以下回调函数获得识别结果：<br>@protocol DecoderDelegate<nsobject><br>@optional</nsobject></li>
<li>(void)decoder:(Decoder <em>)decoder willDecodeImage:(UIImage </em>)image usingSubset:(UIImage *)subset;</li>
<li>(void)decoder:(Decoder <em>)decoder didDecodeImage:(UIImage </em>)image usingSubset:(UIImage <em>)subset withResult:(TwoDDecoderResult </em>)result {<br>NSLog(@”result = %@”, [result text]);<br>}</li>
<li>(void)decoder:(Decoder <em>)decoder failedToDecodeImage:(UIImage </em>)image usingSubset:(UIImage <em>)subset reason:(NSString </em>)reason;</li>
<li>(void)decoder:(Decoder *)decoder foundPossibleResultPoint:(CGPoint)point;</li>
</ul>
<p>@end<br>Trouble Shoot &amp; Tips<br>我在使用中遇到了一些问题，主要是编译的问题。<br>一个是找不到 头文件。解决方法：把用到ZXing的源文件扩展名由.m改成.mm。<br>报错：Undefined symbols for architecture armv7s，解决方法：把ZXingWidget的一个build target参数：”Build Active Architecture Only” 修改成 “NO”.<br>报错：No such file or directory，出现该错误可能是你的Header Search Path写错了，或者就是你的zxing库的目录结构不是我上面强调的，好好检查一下吧。<br>如果你需要生成二维码做测试，推荐一个不错的在线生成二维码的网站：<a href="http://cli.im/" target="_blank" rel="external">http://cli.im/</a><br>ZXing和OpenCV的兼容问题<br>ZXing 2.1 和OpenCV 2.4.3的iOS库有一些兼容问题，他们对C++标准库的版本和编译器版本都有一些需求，造成满足一方了，另一方就编译不通过了。Stackoverflow上有人终于找到了一个让它们和平共处的方法，但是只适用于iOS5.0以上版本。正好我们的App只支持iOS5.0+，所以就搞定了。所以如果你也正好遇到这个问题，可以参考这个贴子。</p>
<p></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2014/01/20/iOS中使用ZXing库/" data-id="cipiddz66000c113zvz6l82bh" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-iOS应用内支付小结" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2014/01/14/iOS应用内支付小结/" class="article-date">
  <time datetime="2014-01-14T13:53:23.000Z" itemprop="datePublished">2014-01-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/01/14/iOS应用内支付小结/">iOS应用内支付小结</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><span></span></p>
<p>我们在今年春节后上线了新的在线智能题库：猿题库。这应该是我们在互联网教育这个创业领域尝试的第二个方向。<br>猿题库现在推出了公务员考试行测和申论 2 个产品，均包括 web, iOS 和 Android 三个平台。这次我们尝试做一个收费的产品，所以在 iOS 端集成了应用内支付（IAP）功能。在开发过程中和上线后，我们遇到了 IAP 中的一些坑，在此分享给各位。<br>IAP 审核相关的坑<br>IAP 开发的详细步骤我写在 另一篇博客 中了。在此主要介绍审核时遇到的问题。<br>IAP 类型错误<br>由于我们是按月付费的产品，所以在设置 IAP 类型时，我没有经验，只是简单设置成了可重复消费 (Consumable) 的 IAP 项目。但是我不知道，苹果对于这种按时间收费的产品，应该使用不可更新的定阅（Non-Renewing Subscription）类型。这个类型设置错误造成了我们 app 的一次审核被拒。<br>IAP 验证逻辑<br>由于苹果在 iOS5.0 以下有 IAP 的 bug，使得攻击者可以伪造支付成功的凭证。而 iOS6.0 的系统在越狱后同样可以伪造凭证，所以我们对于应用内支付，增加了服务器端的验证。<br>服务器端会将支付凭证发给苹果的服务器进行二次验证，以保证凭证是真实有效的。<br>在我们公司的测试服务器中，我们会连接苹果的测试服务器（ <a href="https://sandbox.itunes.apple.com/verifyReceipt" target="_blank" rel="external">https://sandbox.itunes.apple.com/verifyReceipt</a> ）验证。<br>在我们部署在线上的正式服务器中，我们会连接苹果的正式服务器（ <a href="https://buy.itunes.apple.com/verifyReceipt" target="_blank" rel="external">https://buy.itunes.apple.com/verifyReceipt</a> ）验证。<br>我们提交给苹果审核的是正式版，我们以为苹果审核时，我们应该连接苹果的线上验证服务器来验证购买凭证。结果我理解错了，苹果在审核 App 时，只会在 sandbox 环境购买，其产生的购买凭证，也只能连接苹果的测试验证服务器。但是审核的 app 又是连接的我们的线上服务器。所以我们这边的服务器无法验证通过 IAP 购买，造成我们 app 的又一次审核被拒。<br>解决方法是判断苹果正式验证服务器的返回 code，如果是 21007，则再一次连接测试服务器进行验证即可。苹果的 这一篇文档 上有对返回的 code 的详细说明。<br>IAP 上线后的遇到的情况<br>我们在服务器端增加了验证 IAP 是否有效的逻辑。在产品上线后，如我们所料，我们收到了大量的欺骗性购买，这些都被我们的服务器识别出来了，但是我们也遇到了以下这次没有想到的情况:<br>1、由于国内越狱用户的比例比较大 (2012 年底国内越狱比例是 42%), 所以虽然我们服务器会验证购买凭证，但是每天有超过 50% 以上的凭证都是伪造的。同时由于苹果的验证服务器在美国，凭证验证请求响应的时间比较慢，大量的伪造凭证发给苹果服务器，不知道会不会被苹果认为我们是在恶意进行 DDOS。至少我们发现有些时候，验证请求会超时。<br>2、由于国内有许多小白用户，他们的手机从购买时就被渠道商帮忙越狱过了并且安装了 IAP free 插件。所以对于这类用户，他们即使想付费购买，由于系统原有的 IAP 支付功能已经被破坏，所以他们是无法正常付费的。麻烦的是，他们会以为这是我们的 app 的问题，转而给我们的客服打电话投诉。这让我们非常郁闷。<br>3、苹果的验证服务器有时候会出问题，我们发现本来约定好返回的 JSON 数据在有几次返回的居然是一个 XML 格式的文件。造成我们将正常的付费 IAP 凭证验证失败。所以，在服务器记录下所有的验证凭证非常有必要，一来可以防止黑客多次提交同一个成功凭证的重放攻击，二来在需要时可以手工进行再验证。<br>越狱手机可能被黑客窃取购买凭证！！<br>我们发现有一部分用户反馈说已经收到苹果的扣费账单，但是我们从服务器的验证记录看，他上传的凭证却是虚假的。由于这些用户不太多，我们一开始以为是用户在恶意欺骗我们，后来我们让他将苹果的付费账单邮件转发给我们，以及将 itunes 的购买记录截图转发给我们，随着讨论的深入，我们越来越怀疑这里面有一个黑色的产业链。越狱手机的正常购买凭证可能被黑客的恶意程序截获，具体的攻击方式我们讨论了一下，其实就是被 中间人攻击，详细的过程如下:<br>越狱手机的在被破解后，可能从一些破解渠道安装了黑客的恶意程序。<br>黑客将越狱手机所有 https 请求都经过他的中间服务器。<br>当有支付请求时，黑客先将请求发给苹果服务器，待苹果将成功的凭证返回后，黑客将这个凭证替换成假的凭证，完全支付凭证的偷取。<br>或许有人会问，这个凭证拿来有什么用呢 ? 很简单 ，因为苹果为了保护用户的隐私，支付凭证中并不包含任何用户的 apple id 信息，所以我们的 app 和服务器无法知道这个凭证是谁买的，而只能知道这个凭证是真的还是假的。于是黑客就可以用这个凭证，在另外的账号中通知我们完成了购买，而发来的验证凭证又是真实的，所以我们的服务器就会误认为是黑客的账号完成了购买，继而把会员期算在黑客的账号上。<br>再举一个简单的例子，你拿 500 块钱买了顺风优选的 500 元购物券，由于这个购物券是不记名的，所以顺风优选无法知道是谁买的。如果这个购物券在发放过程中被人掉包，那么偷购物券的人就可以拿这个偷来的真购物券来购物，而顺风优选的卡因为是不记名的，所以也无法查证这件事情。在这个例子中，购物券的不记名和苹果的支付凭证无账号信息是同一个道理。<br>鉴于以上情况，考虑到越狱手机不但不能成功支付，还会有安全问题，所以我们在新版中取消了越狱手机中的 IAP 支付功能。<br>所以，请大家还是不要越狱自己的手机，iPhone 手机越狱后风险相当大。实在不值得为了免费玩几个游戏就丢掉安全性。<br>后记<br>中间人攻击的演示<br>iOS 独立开发者 王轲 _IndieBros 在他的博客文章 《使用 mitmproxy 获取 iTunes 11 的 Raw HTTPs Response》 中演示了如何使用中间人攻击来修改 Game Center 游戏数据。王轲还把我的例子白话翻译了一下（可见我还是说得太绕了，囧）：<br>坏人在购买过程中插了一腿，换走了用户的无记名发票（购物小票形象些），然后手持无记名小票伪装成真实顾客或者转手出售获利。<br>关于越狱与盗版<br>不少细心的同学评论纠正我，指出越狱并不等同于使用盗版。确实，如果说严格的定义，越狱只是让 iPhone 获得 root 权限，进而可以做任何事情。如果越狱的同学在越狱后不安装 IAP free 插件，不使用 app sync 插件，不使用任何国内的和非 bigboss 的 cydia 源，不使用任何盗版软件，所有应用都是从 app store 官方网站上下载的话，被黑客攻击的可能性会降低一些。<br>即使这样，由于手机已经被 root 了，苹果的沙盒安全机制失效，所以风险还是很大的。<br>关于越狱用户的比例<br>有同学提出我文章中写的越狱手机比例太高了，想询问数据来源。这个比例主要来自我们自己的 app 的统计信息，以及结合国内的统计工具友盟的 越狱手机比例统计，去年底国内的越狱比例是 42%。</p>
<p></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2014/01/14/iOS应用内支付小结/" data-id="cipiddz5s000b113zwnfwmom4" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-从Facebook看iOS移动端的开发" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2014/01/01/从Facebook看iOS移动端的开发/" class="article-date">
  <time datetime="2014-01-01T13:47:31.000Z" itemprop="datePublished">2014-01-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/01/01/从Facebook看iOS移动端的开发/">从Facebook看iOS移动端的开发</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><span></span></p>
<p>从 Facebook 谈起<br>Facebook 最近绝对是互联网界的新闻明星。它首先是进行了大手笔的收购：2014 年 2 月，Facebook 以 160 亿美元现金加股票，以及 30 亿美元受限制股票福利的方式 收购移动 IM 应用 WhatsApp，总收购成本 190 亿美元。然后是继续发布了新产品：2014 年 2 月，Facebook 发布了一个新的移动端新闻阅读应用 Paper。最后，Facebook 最近还将自己使用的大量工具开源，包括开源了 Paper 的加载效果 Shimmer，LLDB 的增强工具 chisel，以及 Key-Value Observing 工具 KVOController，如果说这些开源工具让程序员如获至宝的话，那么 Facebook 将 Paper 的交互设计工具 Origami 免费开放，则是对广大设计师的福音，极大地方便了移动交互设计工作的开展。<br>2014 年对于 Facebook 来说也是一个值得纪念的日子。因为从 2004 年 2 月 4 日 Facebook 产品上线到现在，Facebook 刚刚走过 10 个年头。10 年前，Facebook 的创始人扎克伯格才 19 岁，是哈佛大学的一名学生。转眼间 10 年后，Facebook 已经成长为全球最大的社交网络，月活跃用户达到 12 亿，市值约 1200 亿美元。<br>业界内大多讨论的话题都围绕在 Facebook 收购 WhatsApp 这件事情上，而作为一个移动开发者，我更加看重 Facebook 发布 Paper 这件事情。因为 Paper 并不是一个简单的应用，它有着非常优秀的交互效果，并且在产品设计和技术上都使用了许多前沿的技术，那就让我们看看，Paper 的开发到底有何不同之处？<br>交互设计<br>我们首先从产品设计上看 Paper 的不同之处。Paper 虽然只是一个新闻客户端，但从大家对 Paper 的评价上，我们发现优秀的交互再一次成为大家关注的焦点。回想那些成功的应用，大多都有着令人心动的交互效果，例如：Tweetie 的下拉刷新，现在基本上成为 iPhone 上内容刷新的标准。Path 跳出来的红心让人心动，很多朋友甚至会没事点那个红心，欣赏那流畅的按钮散开效果。还有 Mailbox，用流畅的手势操作，将邮件管理与任务管理完美结合起来。<br>国外成功的优秀应用也在影响着国内。交互设计不同于平面设计，不能简单地用 Photoshop 展现，而交互设计对于移动应用的成功又异常关键，所以需要花费不少时间来设计，因此产品经理很难兼顾地做交互设计。所以，在国内的一线互联网公司里，交互设计师这个职位慢慢成了移动应用的标配。但是在大部分的非一线互联网公司里面，移动开发的设计仍然停留在由产品经理简单潦草的完成阶段。所以，Facebook 这次 Paper 的成功发布，再一次给移动开发的从业者指出了交互设计的重要性。<br>回顾中国互联网产业的发展我们可以发现，产品经理（Product Manager）这个职位也是最近五、六年才成为互联网公司的标配的，想必在不远的将来，除着交互设计越来越重要，移动交互设计师也会成为每一个互联网公司重要的必备职位。<br>另一方面，由于工具的欠缺，大量的交互设计师的工作效率非常低下，他们为了做出一个新颖的效果常常需要花费大量精力。这次 Facebook 免费开放出基于苹果 Quartz Composer 的增强工具集 Origami，使得交互设计工作得到更好的辅助。而且在 Facebook 的带动下，jQC 1.0 也出现了。jQC 是一个与 Facebook 之前开源的 Origami 兼容的工具，提供了 15 个新的 Patch 来提高 Quartz Composer 的功能。<br>不过另一方面，该工具仍然需要设计师具备一定的基础编码能力，所以对于广大设计师来说，交互设计工具 Origami 对设计师带来的既是机会，同时也是挑战。<br>移动开发技术<br>随着 iOS 依赖管理工具 Cocoapods 和大量第三方开源库成熟起来，业界积累了大量的优秀开源项目。这次 Facebook 开发 Paper 使用了 将近 100 个第三方开源库，极大地减化了自己的应用开发任务。相信随着移动开发的发展，移动开发的生态圈会越来越成熟，基础的开源组件也将将越来越丰富，广大开发者都将从中受益。<br>另一方面，Facebook 的工程师在 Quora 上反馈 说 Paper 在 Xcode 下打开需要 40 多秒钟，编译一次需要 30 分钟。这反映出大量的开源库的使用也给 iOS 集成编译环境 Xcode 提出了新的挑战，相信苹果会花大力气解决 Xcode 的性能问题。<br>总结<br>Facebook 发布的 Paper 让我看到了移动开发领域的快速发展，大量新的工具和开源技术给了设计师和程序员机会和挑战，相信在移动互联网快速发展的浪潮中，会涌现出越来越多优秀的移动应用。谁会是未来移动互联网的霸主？让我们拭目以待。</p>
<p></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2014/01/01/从Facebook看iOS移动端的开发/" data-id="cipiddz68000d113zy30m6jvv" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-用CocoaPods管理iOS程序" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2013/12/30/用CocoaPods管理iOS程序/" class="article-date">
  <time datetime="2013-12-30T13:43:15.000Z" itemprop="datePublished">2013-12-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2013/12/30/用CocoaPods管理iOS程序/">用CocoaPods管理iOS程序</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><span></span></p>
<p>CocoaPods 简介<br>每种语言发展到一个阶段，就会出现相应的依赖管理工具，例如 Java 语言的 Maven，nodejs 的 npm。随着 iOS 开发者的增多，业界也出现了为 iOS 程序提供依赖管理的工具，它的名字叫做：CocoaPods。<br>CocoaPods项目的源码 在 Github 上管理。该项目开始于 2011 年 8 月 12 日，经过多年发展，现在已经成为 iOS 开发事实上的依赖管理标准工具。开发 iOS 项目不可避免地要使用第三方开源库，CocoaPods 的出现使得我们可以节省设置和更新第三方开源库的时间。<br>我在开发猿题库客户端时，使用了 24 个第三方开源库。在没有使用 CocoaPods 以前，我需要:<br>把这些第三方开源库的源代码文件复制到项目中，或者设置成 git 的 submodule。<br>对于这些开源库通常需要依赖系统的一些 framework，我需要手工地将这些 framework 分别增加到项目依赖中，比如通常情况下，一个网络库就需要增加以下 framework: CFNetwork, SystemConfiguration, MobileCoreServices, CoreGraphics, zlib。<br>对于某些开源库，我还需要设置-licucore或者 -fno-objc-arc等编译参数<br>管理这些依赖包的更新。<br>这些体力活虽然简单，但毫无技术含量并且浪费时间。在使用 CocoaPods 之后，我只需要将用到的第三方开源库放到一个名为 Podfile 的文件中，然后执行pod install。<br>CocoaPods 就会自动将这些第三方开源库的源码下载下来，并且为我的工程设置好相应的系统依赖和编译参数。<br>CocoaPods 的安装和使用介绍<br>安装<br>安装方式异常简单 , Mac 下都自带 ruby，使用 ruby 的 gem 命令即可下载安装：<br>$ sudo gem install cocoapods<br>$ pod setup<br>如果你的 gem 太老，可能也会有问题，可以尝试用如下命令升级 gem:<br>sudo gem update –system<br>另外，ruby 的软件源 <a href="https://rubygems.org" target="_blank" rel="external">https://rubygems.org</a> 因为使用的是亚马逊的云服务，所以被墙了，需要更新一下 ruby 的源，使用如下代码将官方的 ruby 源替换成国内淘宝的源：<br>gem sources –remove <a href="https://rubygems.org/" target="_blank" rel="external">https://rubygems.org/</a><br>gem sources -a <a href="https://ruby.taobao.org/" target="_blank" rel="external">https://ruby.taobao.org/</a><br>gem sources -l<br>还有一点需要注意，pod setup在执行时，会输出Setting up CocoaPods master repo，但是会等待比较久的时间。这步其实是 Cocoapods 在将它的信息下载到 ~/.cocoapods目录下，如果你等太久，可以试着 cd 到那个目录，用du -sh *来查看下载进度。你也可以参考本文接下来的使用 cocoapods 的镜像索引一节的内容来提高下载速度。<br>使用 CocoaPods 的镜像索引<br>所有的项目的 Podspec 文件都托管在<a href="https://github.com/CocoaPods/Specs。第一次执行pod" target="_blank" rel="external">https://github.com/CocoaPods/Specs。第一次执行pod</a> setup时，CocoaPods 会将这些podspec索引文件更新到本地的 ~/.cocoapods/目录下，这个索引文件比较大，有 80M 左右。所以第一次更新时非常慢，笔者就更新了将近 1 个小时才完成。<br>一个叫 akinliu 的朋友在 gitcafe 和 oschina 上建立了 CocoaPods 索引库的镜像，因为 gitcafe 和 oschina 都是国内的服务器，所以在执行索引更新操作时，会快很多。如下操作可以将 CocoaPods 设置成使用 gitcafe 镜像：</p>
<p>pod repo remove master<br>pod repo add master <a href="https://gitcafe.com/akuandev/Specs.git" target="_blank" rel="external">https://gitcafe.com/akuandev/Specs.git</a><br>pod repo update<br>将以上代码中的 <a href="https://gitcafe.com/akuandev/Specs.git" target="_blank" rel="external">https://gitcafe.com/akuandev/Specs.git</a> 替换成 <a href="http://git.oschina.net/akuandev/Specs.git" target="_blank" rel="external">http://git.oschina.net/akuandev/Specs.git</a> 即可使用 oschina 上的镜像。<br>使用 CocoaPods<br>使用时需要新建一个名为 Podfile 的文件，以如下格式，将依赖的库名字依次列在文件中即可<br>platform :ios<br>pod ‘JSONKit’,       ‘~&gt; 1.4’<br>pod ‘Reachability’,  ‘~&gt; 3.0.0’<br>pod ‘ASIHTTPRequest’<br>pod ‘RegexKitLite’<br>然后你将编辑好的 Podfile 文件放到你的项目根目录中，执行如下命令即可：<br>cd “your project home”<br>pod install<br>现在，你的所有第三方库都已经下载完成并且设置好了编译参数和依赖，你只需要记住如下 2 点即可：<br>使用 CocoaPods 生成的 .xcworkspace 文件来打开工程，而不是以前的 .xcodeproj 文件。<br>每次更改了 Podfile 文件，你需要重新执行一次pod update命令。<br>查找第三方库<br>你如果不知道 cocoaPods 管理的库中，是否有你想要的库，那么你可以通过 pod search 命令进行查找，以下是我用 pod search json 查找到的所有可用的库：<br>$ pod search json</p>
<p>-&gt; AnyJSON (0.0.1)<br>Encode / Decode JSON by any means possible.</p>
<ul>
<li>Homepage: <a href="https://github.com/mattt/AnyJSON" target="_blank" rel="external">https://github.com/mattt/AnyJSON</a></li>
<li>Source:   <a href="https://github.com/mattt/AnyJSON.git" target="_blank" rel="external">https://github.com/mattt/AnyJSON.git</a></li>
<li>Versions: 0.0.1 [master repo]</li>
</ul>
<p>-&gt; JSONKit (1.5pre)<br>A Very High Performance Objective-C JSON Library.</p>
<ul>
<li>Homepage: <a href="https://github.com/johnezang/JSONKit" target="_blank" rel="external">https://github.com/johnezang/JSONKit</a></li>
<li>Source:   git://github.com/johnezang/JSONKit.git</li>
<li>Versions: 1.5pre, 1.4 [master repo]</li>
</ul>
<p>// … 以下省略若干行<br>关于 Podfile.lock<br>当你执行pod install之后，除了 Podfile 外，CocoaPods 还会生成一个名为Podfile.lock的文件，Podfile.lock 应该加入到版本控制里面，不应该把这个文件加入到.gitignore中。因为Podfile.lock会锁定当前各依赖库的版本，之后如果多次执行pod install 不会更改版本，要pod update才会改Podfile.lock了。这样多人协作的时候，可以防止第三方库升级时造成大家各自的第三方库版本不一致。<br>CocoaPods 的这篇 官方文档 也在What is a Podfile.lock一节中介绍了Podfile.lock的作用，并且指出：<br>This file should always be kept under version control.<br>为自己的项目创建 podspec 文件<br>我们可以为自己的开源项目创建podspec文件，首先通过如下命令初始化一个podspec文件：<br>pod spec create your_pod_spec_name<br>该命令执行之后，CocoaPods 会生成一个名为your_pod_spec_name.podspec的文件，然后我们修改其中的相关内容即可。<br>具体步骤可以参考这两篇博文中的相关内容：<br>《如何编写一个 CocoaPods 的 spec 文件》<br>《Cocoapods 入门》。<br>使用私有的 pods<br>我们可以直接指定某一个依赖的podspec，这样就可以使用公司内部的私有库。该方案有利于使企业内部的公共项目支持 CocoaPods。如下是一个示例：<br>pod ‘MyCommon’, :podspec =&gt; ‘<a href="https://yuantiku.com/common/myCommon.podspec" target="_blank" rel="external">https://yuantiku.com/common/myCommon.podspec</a>‘<br>不更新 podspec<br>CocoaPods 在执行pod install和pod update时，会默认先更新一次podspec索引。使用–no-repo-update参数可以禁止其做索引更新操作。如下所示：<br>pod install –no-repo-update<br>pod update –no-repo-update<br>生成第三方库的帮助文档<br>如果你想让 CococaPods 帮你生成第三方库的帮助文档，并集成到 Xcode 中，那么用 brew 安装 appledoc 即可：<br>brew install appledoc<br>关于 appledoc，我在另一篇博客 《使用 Objective-C 的文档生成工具:appledoc》 中有专门介绍。它最大的优点是可以将帮助文档集成到 Xcode 中，这样你在敲代码的时候，按住 opt 键单击类名或方法名，就可以显示出相应的帮助文档。<br>原理<br>大概研究了一下 CocoaPods 的原理，它是将所有的依赖库都放到另一个名为 Pods 项目中，然后让主项目依赖 Pods 项目，这样，源码管理工作都从主项目移到了 Pods 项目中。发现的一些技术细节有：<br>Pods 项目最终会编译成一个名为 libPods.a 的文件，主项目只需要依赖这个 .a 文件即可。<br>对于资源文件，CocoaPods 提供了一个名为 Pods-resources.sh 的 bash 脚本，该脚本在每次项目编译的时候都会执行，将第三方库的各种资源文件复制到目标目录中。<br>CocoaPods 通过一个名为 Pods.xcconfig 的文件来在编译时设置所有的依赖和参数。</p>
<p></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2013/12/30/用CocoaPods管理iOS程序/" data-id="cipiddz6i000h113z03cojxu5" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-iOS开发如何提高" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2013/12/24/iOS开发如何提高/" class="article-date">
  <time datetime="2013-12-24T13:38:32.000Z" itemprop="datePublished">2013-12-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2013/12/24/iOS开发如何提高/">iOS开发如何提高</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><span></span></p>
<p>阅读博客<br>在现在这个碎片化阅读流行的年代，博客的风头早已被微博盖过。而我却坚持写作博客，并且大量地阅读同行的 iOS 开发博客。博客的文章长度通常在 3000 字左右，许多 iOS 开发知识都至少需要这样的篇幅才能完整地讲解清楚。并且博客相对于书籍来说，并没有较长的出版发行时间，所以阅读博客对于获取最新的 iOS 开发知识有着非常良好的效果。<br>我自己精心整理了国内 40 多位 iOS 开发博主的博客地址列表：<a href="https://github.com/tangqiaoboy/iOSBlogCN，希望大家都能培养起阅读博客的习惯。" target="_blank" rel="external">https://github.com/tangqiaoboy/iOSBlogCN，希望大家都能培养起阅读博客的习惯。</a><br>国外也有很多优秀的 iOS 开发博客，他们整体质量比中文的博客更高，以下是一些推荐的博客地址列表：<br>博客名                        博客地址<br>objc.io                       <a href="http://www.objc.io/" target="_blank" rel="external">http://www.objc.io/</a><br>Ray Wenderlich               <a href="http://www.raywenderlich.com" target="_blank" rel="external">http://www.raywenderlich.com</a><br>iOS Developer Tips           <a href="http://iosdevelopertips.com/" target="_blank" rel="external">http://iosdevelopertips.com/</a><br>iOS Dev Weekly               <a href="http://iosdevweekly.com/" target="_blank" rel="external">http://iosdevweekly.com/</a><br>NSHipster                   <a href="http://nshipster.com/" target="_blank" rel="external">http://nshipster.com/</a><br>Bartosz Ciechanowski       <a href="http://ciechanowski.me" target="_blank" rel="external">http://ciechanowski.me</a><br>Big Nerd Ranch Blog           <a href="http://blog.bignerdranch.com" target="_blank" rel="external">http://blog.bignerdranch.com</a><br>Nils Hayat    <a href="http://nilsou.com/" target="_blank" rel="external">http://nilsou.com/</a><br>另外，使用博客 RSS 聚合工具（例如 Feedly：<a href="http://www.feedly.com/）可以获得更好的博客阅读体验。手机上也有很多优秀的博客阅读工具（我使用的是" target="_blank" rel="external">http://www.feedly.com/）可以获得更好的博客阅读体验。手机上也有很多优秀的博客阅读工具（我使用的是</a> Newsify）。合理地使用这些工具也可以将你在地铁上、睡觉前等碎片时间充分利用上。<br>读书<br>博客的内容通常只能详细讲解一个知识点，而书籍则能成体系地介绍整个知识树。相比国外，中国的书籍售价相当便宜，所以这其实是一个非常划算的提高的方式。建议大家每年至少坚持读完 1 本高质量的 iOS 开发书籍。<br>去年出版的 《iOS 7 Programming Pushing the Limits》 以及 《Objective-C 高级编程：iOS 与 OS X 多线程和内存管理》 都算是不错的进阶方面的读物。顺便打个广告，我自己也在写一本 iOS 进阶方面的图书，年底前应该能上市。<br>看 WWDC 视频<br>由于 iOS 开发在快速发展，每年苹果都会给我们带来很多新的知识。而对于这些知识，第一手的资料就是 WWDC 的视频。<br>通常情况下，一个 iOS 开发的新知识首先会在 WWDC 上被苹果公开，然后 3 个月左右，会有国内外的博客介绍这些知识，再过半年左右，会有国外的图书介绍这些知识。所以如果想尽早地了解这些知识，那么只有通过 WWDC 的视频。<br>现在每年的 WWDC 视频都会在会议过程中逐步放出，重要的视频会带有英文字幕。坚持阅读这些视频不但可以获得最新的 iOS 开发知识，还可以提高英文听力水平。<br>看苹果的官方文档<br>苹果的官方文档相当详尽，对于不熟悉的 API，阅读官方文档也是最直接有效地方式。<br>苹果的文档比较海量，适合选一些重点来阅读，比如人机交互指南就是必读的，而其它的内容可以遇到的时候作为重点资源来查阅。<br>看开源项目的代码<br>大家一定有这样的感受，很多时候用文字讲解半天，还不如写几行代码来得直观。阅读优秀的开源项目代码，不但可以学习到 iOS 开发本身的基本知识，还能学习到设计模式等软件架构上的知识。<br>如果读者能够参与到开源项目的开发中，则能进一步提高自己的能力。<br>多写代码，多思考<br>知识的积累离不开实践和总结，我认为 iOS 代码量如果没有超过 10 万行，是不能称得上熟悉 iOS 开发的。某些在校的学生，仅仅做了几个 C++ 的大作业，就在求职简历里面写上 “精通 C++”，则真是让人哭笑不得。<br>在多写代码的同时，我们也要注意不要 “ 重复造轮子 “，尽量保证每次写的代码都能具有复用性。在代码结构因为业务需求需要变更时，及时重构，在不要留下技术债的同时，我们也要多思考如何设计应用架构，能够保证满足灵活多变的产品需求。<br>在多次重构和思考的过程中，我们就会慢慢积累出一类问题的 “最佳实践” 方式，成为自己宝贵的经验。<br>多和同行交流<br>有些时候遇到一些难解的技术问题，和同行的几句交流就可能让你茅塞顿开。。另外常见的技术问题通常都有人以前遇到过，简单指导几句就能让你一下子找到正确的解决方向。<br>国内开发者之间的交流，可以通过论坛，微博，QQ 群等方式来进行。另外各大公司有时候会办技术沙龙，这也是一个认识同行的好机会。<br>需要特别提醒的是，和国内开发者之前交流要注意讨论质量，有一些论坛和 QQ 群讨论质量相当低下，提的问题都是能通过简单 Google 获得的，这种社区一定要远离，以提高自己的沟通效率。<br>除了在国内的技术社区交流，建议读者可以去国外的 stackoverflow：<a href="http://www.stackoverflow.com" target="_blank" rel="external">http://www.stackoverflow.com</a> 上提问或回答问题。<br>分享<br>值得尝试的分享方式有：发起一个开源项目、写技术博客、在技术会议上做报告。这几种方式都比较有挑战，但是如果能大胆尝试，肯定会有巨大的收获。</p>
<p></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2013/12/24/iOS开发如何提高/" data-id="cipiddz6d000f113zkwrrnetp" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">__('next') &raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/01/">January 2015</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/01/">January 2014</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/12/">December 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/11/">November 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/10/">October 2013</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2013/09/">September 2013</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/02/15/搞定iOS与js交互/">搞定iOS与js交互</a>
          </li>
        
          <li>
            <a href="/2016/01/28/移动直播开发技术介绍/">移动直播开发技术介绍</a>
          </li>
        
          <li>
            <a href="/2016/01/22/Bilibili开源的直播框架/">Bilibili开源的直播框架</a>
          </li>
        
          <li>
            <a href="/2016/01/14/iOS三种视屏录制方式/">iOS三种视屏录制方式</a>
          </li>
        
          <li>
            <a href="/2015/01/07/iOS-JSON解析四种方法/">iOS_JSON解析四种方法</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>